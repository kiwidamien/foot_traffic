{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary of findings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The [notebook](./location_analysis.ipynb) containing the analysis is quite long, and tries to show my thought process as I tackled this problem. This notebook summarizes my high level findings. \n",
    "\n",
    "\n",
    "**Note:**\n",
    "All section references (i.e. $\\S$X.X) are references to the [analysis notebook](./location_analysis.ipynb), not sections in this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data issues"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data was mostly clean, with only a couple of issues:\n",
    "- direction data was missing for 58 rows\n",
    "- There was one significant outlier in the response variable, which was dropped\n",
    "\n",
    "The validation set was scanned to see if there were data points that would be obvious errors (which we would not want to penalize our model for) but the validation set looked clean."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Depedence of rows -- 25 stores with multiple measurements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The biggest issue that came up when dealing with this dataset was the time series nature of the data. Specifically, by looking at features that we would expect to remain constant for a store in a particular day, such as the region, the number of seats, and the number of parking spaces, the market type, and the shop type, I was able to determine there were likely 25 distinct stores, with repeated measurements on different days.\n",
    "\n",
    "Given more time, I would have liked to see if I could make predictions about the store using the features that are constant on the store. One commonly used technique is `mixed-effect modeling`, such as the `statsmodels` package (although many different hierarichal models would also work). Hierarical models don't feature within the `sklearn` toolkit. With the small number of stores, we want to be careful that we don't have the extra features simply \"encode\" which store we are in, so that we can be confident our model will generalize out of sample. \n",
    "\n",
    "Mixed effect models, where the fixed effects were the store, and the random effects were the different days, would also make a lot of sense for the application. When a new store is inserted, we probably don't care about the effect of a store on a particular day, but are likely to care about what the effect of the store averaged over many days (as the store will continue to exist once it is constructed). Our prompt on the task was to look specifically at predicting the `deltaRevenue` on each day, so this would not have satisfied the task.\n",
    "\n",
    "There are variables such as `Parking_slots` and `Seats` that would tell us about the size of a store, which are plausibly very strong predictors. I would like to try making a simple model that uses some of these features on the individual stores to predict the average `deltaRevenue`, but this would need to be done using a statistical criteria (e.g. AIC or WAIC) instead of cross-validation, because of the small number of stores.\n",
    "\n",
    "As a very specific example, there is only one store with 13 parking slots, as we can see in the table below. There are multiple stores with `0` parking slots, but they are spread across different regions. There _is_ very likely a signal to extract here, but I did not have time to extract it, as many of the standard `sklearn` pipelines are designed with independent rows in mind. To do store level features would mean using a hierarichal model package, _or_ putting a lot of time into doing my own custom cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-31T06:15:04.170591Z",
     "start_time": "2021-03-31T06:15:04.103652Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>directionCode</th>\n",
       "      <th>type_dtsf</th>\n",
       "      <th>mtype</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Parking_slots</th>\n",
       "      <th>region</th>\n",
       "      <th>Seats</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"7\" valign=\"top\">0</th>\n",
       "      <th>1</th>\n",
       "      <th>162</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">3</th>\n",
       "      <th>158</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>210</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <th>86</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">8</th>\n",
       "      <th>73</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <th>1</th>\n",
       "      <th>100</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">15</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">1</th>\n",
       "      <th>53</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <th>1</th>\n",
       "      <th>94</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <th>5</th>\n",
       "      <th>57</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <th>2</th>\n",
       "      <th>92</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">31</th>\n",
       "      <th>1</th>\n",
       "      <th>120</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <th>131</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <th>9</th>\n",
       "      <th>52</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <th>6</th>\n",
       "      <th>100</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <th>7</th>\n",
       "      <th>60</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <th>2</th>\n",
       "      <th>64</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>217</th>\n",
       "      <th>1</th>\n",
       "      <th>188</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>600</th>\n",
       "      <th>7</th>\n",
       "      <th>80</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1000</th>\n",
       "      <th>5</th>\n",
       "      <th>126</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1500</th>\n",
       "      <th>9</th>\n",
       "      <th>110</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1900</th>\n",
       "      <th>9</th>\n",
       "      <th>548</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2500</th>\n",
       "      <th>5</th>\n",
       "      <th>119</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            directionCode  type_dtsf  mtype\n",
       "Parking_slots region Seats                                 \n",
       "0             1      162                1          1      1\n",
       "              3      158                1          1      1\n",
       "                     210                1          1      1\n",
       "              4      86                 1          1      1\n",
       "              8      73                 1          1      1\n",
       "                     105                1          1      1\n",
       "                     151                1          1      1\n",
       "13            1      100                1          1      1\n",
       "15            1      53                 1          1      1\n",
       "                     106                1          1      1\n",
       "17            1      94                 1          1      1\n",
       "21            5      57                 1          1      1\n",
       "30            2      92                 1          1      1\n",
       "31            1      120                1          1      1\n",
       "              10     131                1          1      1\n",
       "36            9      52                 1          1      1\n",
       "37            6      100                1          1      1\n",
       "61            7      60                 1          1      1\n",
       "80            2      64                 1          1      1\n",
       "217           1      188                1          1      1\n",
       "600           7      80                 0          1      1\n",
       "1000          5      126                1          1      1\n",
       "1500          9      110                1          1      1\n",
       "1900          9      548                1          1      1\n",
       "2500          5      119                1          1      1"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.read_csv('data/train.csv')\n",
    "uniqueness = train.groupby(['Parking_slots', 'region', 'Seats']).nunique()\n",
    "\n",
    "## What are the unique properities (i.e. only have one value for a given (parking, seats, region) combo)?\n",
    "uniqueness.loc[:, uniqueness.max()==1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I did make one attempt to use a store specific feature, `mtype` (market type) in $\\S$3.4. I attempted to encode this feature using _target encoding_. Because of the pipeline issues, I did this in a way that did leak data across the different stores. However, even with this leakage, the addition of the encoded feature did not improve the results.\n",
    "\n",
    "Because we want this data to generalize to _new_ stores, I did ensure that for my modeling work that I used `GroupKFold` validation for setting up my grid search. This ensured that no \"store\" (i.e. parking, seat, region combo) would appear in both my training and hold out fold. This was important as we want our results to generalize to new stores.\n",
    "\n",
    "\n",
    "### Summary\n",
    "\n",
    "* There were 25 independent stores, which lead to some features being very strongly correlated\n",
    "* The cross validation method was modified to make sure we could generalize, by ensuring that we did not have a store appear in the training and validation folds\n",
    "* The set of features that were constant on a store were (`Parking_slots`, `Seats`, `region`, `mtype`, `type_dtsf`, `directionCode`)\n",
    "* Building a hierarchical or mixed-effects model would probably be the right way of approaching this, and would allow us to measure the store specific properties (fixed effects), while averaging over the daily effects. Assuming these are not \"pop-up\" shops, we are likely more interested in the long-term effects than the value of `deltaRevenue` on a specific day.\n",
    "* One attempt to build a model using a _crude_ version of mixed effect modeling (making a categorical feature into one correlated with the target) was tried in $\\S$3.4 and made model performance worse.\n",
    "* I would expect there to be strong signal in `Parking_slots` and / or `region` (one relates to the size of the store, the other would presumably be encoded with affluence). I tried working with `region` a little bit, but did not have time to investigate `Parking_slots`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-31T06:00:44.352646Z",
     "start_time": "2021-03-31T06:00:44.329925Z"
    }
   },
   "source": [
    "## 3. Feature and data leakage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In $\\S$1.5 we look at how some of the features are related to the target, `deltaRevenue`.\n",
    "\n",
    "In that section, I discuss some of the issues about what features we are able to use without worrying about data leakage. If the goal of this model is to make new predictions about a store that does not yet exist, we will not have access to the `totalRevenue` on a day before we have built the store! \n",
    "\n",
    "This problem exists for many of features (e.g. the number of transactions has the same problem, the number of activities might be set by external forces, or might not be available either). We don't know anything about how the features `r_*` are measured either, so we don't know if there is data leakage there.\n",
    "\n",
    "Here is a copy of the relevant discussion (copied and pasted):\n",
    "\n",
    "---\n",
    "---\n",
    "\n",
    "A few callouts here:\n",
    "\n",
    "1. Both `transactCount` and `totalRevenue` are positively correlated with `deltaRevenue`. Basically the more your store does in revenue, the more we expect the `deltaRevenue` to be\n",
    "2. Depending on whether `transactCount` and `totalRevenue` are of the _existing_ store or the _new_ store, it may or may not be fair to include it. In both cases, we don't actually have the measurement of transations of revenue until the day in question, so we wouldn't be able to put measured values into our model!\n",
    "  - If it is for the transactions of the _existing store_, we probably should not include it. We have measurements of the typical revenue of the store _after_ the new store was put in, but we don't know what it was before. The thing that we are trying to measure is the effect of putting the store in!\n",
    "    - We could do something a more complicated: use `deltaRevenue` to reconstruct the revenue _before_ the new store moved in, determine the previous revenue `train['counterFactualRevenue'] = train['totalRevenue']/(1 + train['deltaRevenue']/100)`. Then we could use estimates of the existing store performance for the feature `counterFactualRevenue`, and we don't have `totalRevenue` as a feature at all. We would also do this transformation on the validation set (it looks circular but isn't as long as we don't use `totalRevenue` -- it is assuming the way we would use this is projecting the current revenue of the store in the absence of the change).\n",
    "    - If we did this, we would need to investigate how the `deltaRevenue` was measured, to ensure that we didn't end up with a circular definition!\n",
    "  - If it for the transactions of the _new store_, we still don't have the data, but we could use the information strategically. For example, if we know that we need to do at least 100 transactions for the new store to be viable, it makes sense to ask \"we are going to put a store here, and have at least 100 transactions in it\" to assess the impact and check we do not cannibalise the existing store too much.\n",
    "  - At the moment, I will include these features, as it isn't clear what the meaning of them are, so I will allow myself to include them. I am just including notes of how we would modify these fields\n",
    "3. We should check for colinearity in general, but in particular between `transactCount` and `totalRevenue` (we would expect Revenue and Transactions to be strongly correclated!)\n",
    "4. We have masked the effect of time -- a lot of the scatter can come about by looking at the same store on different days (especially for `region`, `Parking_slots` , and `Seats`)\n",
    "5. `totalActivitiesRefcircle` and `totalCustomerRefcircle` also have roughly the same shape as `transactCount` and `totalRevenue` (at least just looking at it on this scale).\n",
    "\n",
    "Let's look at possible correlations between `totalActivitiesRefcircle`, `totalCustomerRefcircle`, `transactCount`, and `totalRevenue`:\n",
    "\n",
    "---\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I did end up using these features, as I didn't get an instruction that there was concern about leakage, but wanted to call it out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Feature transformations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Features that were very large, and right skewed (right tailed) were logged. This bought the range of the numbers back down to managable values, eliminated the skew, and made sense for linear models in terms of the application (if we are modeling percentage change in revenue, this is an _additive_ effect on the log of revenue). These were\n",
    "- `totalRevenue`\n",
    "- `totalActivitiesRefcircle'\n",
    "- `transactCount`\n",
    "- `totalCustomersRefcircle`\n",
    "\n",
    "\n",
    "The total number of customers was found to be strongly correlated with activities, so it was dropped ($\\S$ 1.5)\n",
    "\n",
    "In a sample of a couple of random stores, we saw that there was not an overall trend in our individual stores sales against date. We saw in $\\S$1.5 that some of the market types, like *Rural* and *Suburban* seemed to have a different `deltaRevenue`, but were less well respresented in our data. \n",
    "\n",
    "In $\\S$1.6 we looked at days of the week, and saw surprising little effect on `deltaRevenue`. Monday has a slightly wider interquartile range, Saturday has a slightly smaller IQR, and visually Tuesday seemed to have a longer tail of outliers (but I did not test this for significance). I did not see a strong day of week or weekend pattern, at least on the target.\n",
    "\n",
    "There was a strong relationship between holidays, day of week, and `deltaRevenue`, although holiday rows account for approximately 6% of the data.\n",
    "\n",
    "Other transformations, like whether to standard scale or not, or how to encode categorical variables, were done at the level of the individual models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I looked at a Lasso model and a variety of different RandomForest models. I was trying to pick models that would do a good job with feature selection, as I was still worried about the relatively small number of stores in our data set. Since the purpose of this model was to generalize to new stores, I wanted to be pretty aggressive about eliminating features -- especially those correlated directly with the store (I was less concerned about eliminating features that changed daily, like revenue or `r_*` features).\n",
    "\n",
    "For the linear model Lasso, I performed standard scaling on the features, so that regularization was applied on \"fair\" footing. For tree based models, I skipped this as it isn't necessary (the ordining of points along a particular feature matter, but the specific values do not).\n",
    "\n",
    "We tried four models, all selected with the goal of being able to do feature reduction. The models were:\n",
    "\n",
    "\n",
    "1. **LASSO** simple Lasso, with cross-validation to determine regularization\n",
    "2. **Basic forest**, a random forest, with categorical variables one-hot encoded\n",
    "3. **Reduced forest**, a random forest on a reduced set of features, to try and counteract the wide variance seen between folds in model 2\n",
    "4. **Advanced forest**, a random tree, using feature selection (via 2) and trying to use the target to determine the encoding for the market type (done in a way that leaked data).\n",
    "\n",
    "I found model 2 performed the best. Model 3 was similar, with slightly fewer parameters, but both models 2 and 3 had large variations between the folds. They were statistically indistinguishable; I kept model 2 because I had built out more of the pipeline with it in mind. The high degree of variability between folds did indicate that I was overfitting.\n",
    "\n",
    "The extra information from encoding the market category in model 4 made the model worse. This suggests that we really were encoding information about the specific stores when training, and we cannot use this feature without doing a lot more modeling.\n",
    "\n",
    "We move to validation with the **Basic forest**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We used the **Basic forest** model\n",
    "* When doing GroupKFold validation, we were getting $R^2$ values of approximately 0.23 (with large variation between folds)\n",
    "* It was clear there was still overfitting, as looking score of the best model on the full training data set gave a score of about 0.66 (compared to 0.23 out of sample).\n",
    "* The validation set was a lot worse, with $R^2\\approx 0.12$ \n",
    "\n",
    "It would seem that aiming for simpler models would still be better, although attempts to reduce the feature size didn't give better out-of-sample results (e.g. the **reduced forest** performed worse than the original model!). \n",
    "\n",
    "The biggest gains in this exercise are probably trying to measure the effect of the stores better. The crude way of doing this is to construct features of the stores based on Parking slots and seats, and this is probably the next thing I would try.\n",
    "\n",
    "We may do even better (and perhaps it might be a more important question to answer) by doing a mixed-effects model, where we are not trying to answer what the  `deltaRevenue` is on a given day (as a non-popup store is a long term commitment), but instead treat the different days as random effects, and focus on the fixed effects of a given store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hela",
   "language": "python",
   "name": "hela"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
