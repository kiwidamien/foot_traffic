{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-31T05:36:06.829937Z",
     "start_time": "2021-03-31T05:36:05.618225Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-31T05:36:07.577388Z",
     "start_time": "2021-03-31T05:36:06.832113Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, GroupKFold, GridSearchCV\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "import category_encoders as ce"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Where to put a store?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $\\S$1. EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $\\S$1.1 EDA : Exploratory data analysis split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load up the training data, and do some sanity checks. The big things that I am looking for are\n",
    "* data quality issues (is there missing data, and if so, how much)?\n",
    "* how much data is there?\n",
    "\n",
    "If we have very little data, we need to ensure that our EDA doesn't contribute to overfitting by us making decisions on data that survives through each cross-fold of validation. If we have a lot of training data, or we expect to add data at a significant rate, we can worry less about this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-31T05:36:07.649242Z",
     "start_time": "2021-03-31T05:36:07.579618Z"
    }
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv('data/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-31T05:36:07.662229Z",
     "start_time": "2021-03-31T05:36:07.651371Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "directionCode    58\n",
       "dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#number of nulls?\n",
    "nulls = train.isna().sum()\n",
    "nulls[nulls > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-31T05:36:07.682273Z",
     "start_time": "2021-03-31T05:36:07.663982Z"
    }
   },
   "outputs": [],
   "source": [
    "# Make the training data actually a date\n",
    "train['date'] = pd.to_datetime(train['date'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, so 58 missing values in the direction code. We also want a notion of how many rows of data we have:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-31T05:36:07.687559Z",
     "start_time": "2021-03-31T05:36:07.684011Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5059"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $\\S$1.2 EDA: Independent units of data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have 5059 rows of data, but the data may not be independent. For example, we have date information -- but if the data was collected from the same store on multiple days, those rows are not independent. In an extreme case, we might have only one store being measured over and over again, which wouldn't help us generalize to new stores! If there was only one store in the data set:\n",
    "\n",
    "* We **could** use the data set to make predictions on new dates in the data set\n",
    "* We should **not** try to use the data set to make predictions about new stores\n",
    "\n",
    "We can tell from the multiple regions that the number of stores is bigger than one (a store is assumed to be in only one \"region\"). \n",
    "\n",
    "\n",
    "This is important because it influences our cross-validation design. The goal is to make predictions about opening a **new store**, where we have no previous information. If we train on data that has a store in the training _and_ cross-validation folds, then data can leak -- we will know information about an existing store in the training set when looking at our cross-validation fold, and information about the store might get encoded in otherwise marginal features. This will lead to us being overly confident when generalizing to new stores.\n",
    "\n",
    "_If_ we have the same store repeated multiple times, the correct thing to do is to use grouped cross validation, where each store belongs to a single fold. \n",
    "\n",
    "If there was a `store_id` this would be easy to determine. Unfortunately we don't have that. Our goal instead will be to look at the number of rows of data in a given region on a given day. If this is constant, _and_ we see features that would not change day-to-day (e.g. number of parking spots, number of seats), we have confidence that we are looking the same store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-31T05:36:07.707614Z",
     "start_time": "2021-03-31T05:36:07.689111Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>region</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2018-01-10</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2018-01-11</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2018-01-12</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2018-02-10</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2018-02-11</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2370</th>\n",
       "      <td>2020-12-02</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2371</th>\n",
       "      <td>2020-12-02</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2372</th>\n",
       "      <td>2020-12-02</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2373</th>\n",
       "      <td>2020-12-02</td>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2374</th>\n",
       "      <td>2020-12-02</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2375 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           date  region  count\n",
       "0    2018-01-10       8      3\n",
       "1    2018-01-11       8      3\n",
       "2    2018-01-12       8      3\n",
       "3    2018-02-10       8      3\n",
       "4    2018-02-11       8      3\n",
       "...         ...     ...    ...\n",
       "2370 2020-12-02       5      3\n",
       "2371 2020-12-02       6      1\n",
       "2372 2020-12-02       7      2\n",
       "2373 2020-12-02       9      3\n",
       "2374 2020-12-02      10      1\n",
       "\n",
       "[2375 rows x 3 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stores_by_date_and_region = train.groupby(['date', 'region']).deltaRevenue.count().reset_index().rename(columns={'deltaRevenue': 'count'})\n",
    "stores_by_date_and_region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-31T05:36:07.750310Z",
     "start_time": "2021-03-31T05:36:07.712893Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>region</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>354.0</td>\n",
       "      <td>4.115819</td>\n",
       "      <td>2.556030</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>293.0</td>\n",
       "      <td>1.815700</td>\n",
       "      <td>0.388392</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>284.0</td>\n",
       "      <td>1.869718</td>\n",
       "      <td>0.337207</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>261.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>227.0</td>\n",
       "      <td>1.621145</td>\n",
       "      <td>0.925113</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>213.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>136.0</td>\n",
       "      <td>1.426471</td>\n",
       "      <td>0.496392</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>365.0</td>\n",
       "      <td>2.953425</td>\n",
       "      <td>0.247005</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>150.0</td>\n",
       "      <td>2.220000</td>\n",
       "      <td>0.975333</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>92.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        count      mean       std  min  25%  50%  75%  max\n",
       "region                                                    \n",
       "1       354.0  4.115819  2.556030  1.0  2.0  2.0  7.0  7.0\n",
       "2       293.0  1.815700  0.388392  1.0  2.0  2.0  2.0  2.0\n",
       "3       284.0  1.869718  0.337207  1.0  2.0  2.0  2.0  2.0\n",
       "4       261.0  1.000000  0.000000  1.0  1.0  1.0  1.0  1.0\n",
       "5       227.0  1.621145  0.925113  1.0  1.0  1.0  3.0  3.0\n",
       "6       213.0  1.000000  0.000000  1.0  1.0  1.0  1.0  1.0\n",
       "7       136.0  1.426471  0.496392  1.0  1.0  1.0  2.0  2.0\n",
       "8       365.0  2.953425  0.247005  1.0  3.0  3.0  3.0  3.0\n",
       "9       150.0  2.220000  0.975333  1.0  1.0  3.0  3.0  3.0\n",
       "10       92.0  1.000000  0.000000  1.0  1.0  1.0  1.0  1.0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stores_by_date_and_region.groupby('region')['count'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So regions 4, 6, and 10 only show one store consistently. Let's see if we can plausibly say it is the _same_ store by looking at some features that should not change day to day. The easiest of these are \"seats\" and \"parking spots\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-31T05:36:07.764196Z",
     "start_time": "2021-03-31T05:36:07.753288Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "region  Seats  Parking_slots\n",
       "4       86     0                261\n",
       "6       100    37               213\n",
       "10      131    31                92\n",
       "Name: deltaRevenue, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[train.region.isin([4, 6, 10])].groupby(['region', 'Seats', 'Parking_slots']).deltaRevenue.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, it is plausable that these really are the same store (it isn't proof, but is highly indicative) -- otherwise we have a lot of different stores that just happen to have the same number of seats and parking_slots in a region!\n",
    "\n",
    "Let's investigate deeper. Let's ask \n",
    "\n",
    "> \"what are the range of values for `(region, parking slots, seats)` when we scan over the dates?\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-31T05:36:07.777235Z",
     "start_time": "2021-03-31T05:36:07.766109Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "region  Seats  Parking_slots\n",
       "1       53     15               329\n",
       "        94     17               155\n",
       "        100    13               155\n",
       "        106    15               353\n",
       "        120    31               155\n",
       "        162    0                155\n",
       "        188    217              155\n",
       "2       64     80               293\n",
       "        92     30               239\n",
       "3       158    0                284\n",
       "        210    0                247\n",
       "4       86     0                261\n",
       "5       57     21               227\n",
       "        119    2500              70\n",
       "        126    1000              71\n",
       "6       100    37               213\n",
       "7       60     61               136\n",
       "        80     600               58\n",
       "8       73     0                360\n",
       "        105    0                359\n",
       "        151    0                359\n",
       "9       52     36                92\n",
       "        110    1500             150\n",
       "        548    1900              91\n",
       "10      131    31                92\n",
       "Name: date, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stores_in_training = train.groupby(['region', 'Seats', 'Parking_slots']).date.nunique()\n",
    "stores_in_training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This suggests that what we actually have is \n",
    "* 7 stores in region 1,\n",
    "* 2 stores in region 2,\n",
    "* 2 stores in region 3,\n",
    "* 1 store in region 4\n",
    "* 3 stores in region 5\n",
    "* 1 store in region 6\n",
    "* 2 stores in region 7\n",
    "* 3 stores in region 8\n",
    "* 3 stores in region 9\n",
    "* 1 store in region 10\n",
    "\n",
    "The different stores have different amounts of data (e.g. the stores in region 1 have data for 155 days, 329 days, or 353 days).\n",
    "\n",
    "### Bottom line\n",
    "\n",
    "**We cannot treat the rows of data as independent**. When doing cross-validation, we must ensure that each store is either in the fold, or out -- no having some rows of a store being in the training fold, and some being in the validation set.\n",
    "\n",
    "\n",
    "I also need to check that the validation set does not contain data leakage. Usually I would not load validation data until the process was complete. Here I am looking to see if the \"other days\" from the existing stores are in the validation set ... if so, this would not be a design that would generalize to new stores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-31T05:36:07.788483Z",
     "start_time": "2021-03-31T05:36:07.779646Z"
    }
   },
   "outputs": [],
   "source": [
    "validation = pd.read_csv('data/validation.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-31T05:36:07.797686Z",
     "start_time": "2021-03-31T05:36:07.789797Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "region  Seats  Parking_slots\n",
       "4       97     0                258\n",
       "5       92     0                 71\n",
       "7       78     30               136\n",
       "9       130    38                92\n",
       "Name: date, dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stores_in_validation = validation.groupby(['region', 'Seats', 'Parking_slots']).date.nunique()\n",
    "stores_in_validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, none of these stores overlap (ðŸ˜Œ). I don't have to worry that my data is going to leak! Let's just explicitly verify that there are no stores \"in common\", assuming that `(region, Seats, Parking_slots`) uniquely identifies a stores:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-31T05:36:07.810471Z",
     "start_time": "2021-03-31T05:36:07.799299Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>region</th>\n",
       "      <th>Seats</th>\n",
       "      <th>Parking_slots</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [region, Seats, Parking_slots, date]\n",
       "Index: []"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stores_in_validation.reset_index().merge(stores_in_training.reset_index())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No overlap!!\n",
    "\n",
    "### Conclusion of independent data sets\n",
    "\n",
    "- we have many data points _per store_; we should not treat the rows as independent\n",
    "  - in particular, we have 5059 rows, but only **25** stores in our data set\n",
    "- the validation set is correctly designed (contains no stores from the training set)\n",
    "- in lieu of a store_id, we will assume that `(region, parking, seats)` uniquely identify a store, and generate a store id from it\n",
    "- we will use grouped cross-validation to help us generalize our results better\n",
    "- the validation dataframe will be deleted to ensure we don't accidentally peek at the results (verifying design seems like a legitimate case for peaking)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-31T05:36:07.814395Z",
     "start_time": "2021-03-31T05:36:07.812004Z"
    }
   },
   "outputs": [],
   "source": [
    "del validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-31T05:36:07.929908Z",
     "start_time": "2021-03-31T05:36:07.816157Z"
    }
   },
   "outputs": [],
   "source": [
    "train['store_id'] = train.apply(lambda row: (row.region, row.Parking_slots, row.Seats), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $\\S$1.3 EDA: missing data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-30T00:06:12.774818Z",
     "start_time": "2021-03-30T00:06:12.694772Z"
    }
   },
   "source": [
    "We saw about that there was some missing data in only one column, the `directionCode`. Presumably these are encodings of the four cardinal directions, but let's check:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-31T05:36:07.936997Z",
     "start_time": "2021-03-31T05:36:07.931394Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0    0.301641\n",
       "3.0    0.278711\n",
       "4.0    0.218620\n",
       "2.0    0.189563\n",
       "NaN    0.011465\n",
       "Name: directionCode, dtype: float64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.directionCode.value_counts(dropna=False, normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like the four cardinal points, with slight overweighting on directions 1 and 3. If this turns out to be an important feature, we can impute the values with 1.0 (the most common value) or randomly impute and do many times (\"bootstrapping\" to measure the uncertainty). \n",
    "\n",
    "\n",
    "Let's check the \"store_id\" argument, and ask if rows of the same store has a directionCode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-31T05:36:07.950194Z",
     "start_time": "2021-03-31T05:36:07.938712Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "store_id        directionCode\n",
       "(1, 0, 162)     3.0              155\n",
       "(1, 13, 100)    1.0              155\n",
       "(1, 15, 53)     4.0              329\n",
       "(1, 15, 106)    2.0              353\n",
       "(1, 17, 94)     4.0              155\n",
       "(1, 31, 120)    3.0              155\n",
       "(1, 217, 188)   4.0              155\n",
       "(2, 30, 92)     1.0              239\n",
       "(2, 80, 64)     3.0              293\n",
       "(3, 0, 158)     4.0              284\n",
       "(3, 0, 210)     2.0              247\n",
       "(4, 0, 86)      1.0              261\n",
       "(5, 21, 57)     3.0              227\n",
       "(5, 1000, 126)  3.0               71\n",
       "(5, 2500, 119)  1.0               70\n",
       "(6, 37, 100)    1.0              213\n",
       "(7, 61, 60)     1.0              136\n",
       "(7, 600, 80)    NaN               58\n",
       "(8, 0, 73)      1.0              360\n",
       "(8, 0, 105)     2.0              359\n",
       "(8, 0, 151)     3.0              359\n",
       "(9, 36, 52)     4.0               92\n",
       "(9, 1500, 110)  3.0              150\n",
       "(9, 1900, 548)  4.0               91\n",
       "(10, 31, 131)   1.0               92\n",
       "Name: directionCode, dtype: int64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.groupby('store_id').directionCode.value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interesting, all 58 missing values are associated with a single store. It might be legitimately missing, or this store may not face one of the cardinal directions. We don't want to weigh a direction more simply because a store has more measurements, so if we do need to impute, we should base it off the distribution of directions that *stores* face, not the distribution of directions that *measurements* take. \n",
    "\n",
    "That is, what I really want is the frequency of directionCodes from _this_ table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-31T05:36:07.959112Z",
     "start_time": "2021-03-31T05:36:07.952218Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "directionCode\n",
       "1.0    8\n",
       "2.0    3\n",
       "3.0    7\n",
       "4.0    6\n",
       "NaN    1\n",
       "Name: store_id, dtype: int64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.groupby('directionCode', dropna=False).store_id.nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This would still suggest that if the `directionCode` was an important feature, we would impute it using direction 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $\\S$1.4 EDA: are categorical variables unique per \"store_id\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have some categorical variables which seem to be a description of the market or area (namely `mtype` and `type_dfsf`). We would hope these are constant for a particular store, but let's check!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-31T05:36:07.984038Z",
     "start_time": "2021-03-31T05:36:07.960903Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mtype</th>\n",
       "      <th>type_dtsf</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>store_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>(1, 0, 162)</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(1, 13, 100)</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(1, 15, 53)</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(1, 15, 106)</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(1, 17, 94)</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(1, 31, 120)</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(1, 217, 188)</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(2, 30, 92)</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(2, 80, 64)</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(3, 0, 158)</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(3, 0, 210)</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(4, 0, 86)</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(5, 21, 57)</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(5, 1000, 126)</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(5, 2500, 119)</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(6, 37, 100)</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(7, 61, 60)</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(7, 600, 80)</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(8, 0, 73)</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(8, 0, 105)</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(8, 0, 151)</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(9, 36, 52)</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(9, 1500, 110)</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(9, 1900, 548)</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(10, 31, 131)</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                mtype  type_dtsf\n",
       "store_id                        \n",
       "(1, 0, 162)         1          1\n",
       "(1, 13, 100)        1          1\n",
       "(1, 15, 53)         1          1\n",
       "(1, 15, 106)        1          1\n",
       "(1, 17, 94)         1          1\n",
       "(1, 31, 120)        1          1\n",
       "(1, 217, 188)       1          1\n",
       "(2, 30, 92)         1          1\n",
       "(2, 80, 64)         1          1\n",
       "(3, 0, 158)         1          1\n",
       "(3, 0, 210)         1          1\n",
       "(4, 0, 86)          1          1\n",
       "(5, 21, 57)         1          1\n",
       "(5, 1000, 126)      1          1\n",
       "(5, 2500, 119)      1          1\n",
       "(6, 37, 100)        1          1\n",
       "(7, 61, 60)         1          1\n",
       "(7, 600, 80)        1          1\n",
       "(8, 0, 73)          1          1\n",
       "(8, 0, 105)         1          1\n",
       "(8, 0, 151)         1          1\n",
       "(9, 36, 52)         1          1\n",
       "(9, 1500, 110)      1          1\n",
       "(9, 1900, 548)      1          1\n",
       "(10, 31, 131)       1          1"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "train.groupby('store_id')[['mtype', 'type_dtsf']].nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yes, these are unique variables for a store! We could concatanate them onto the store_id key, but there is no need at the moment. Let's also see what the distribution in values is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-31T05:36:07.992288Z",
     "start_time": "2021-03-31T05:36:07.985746Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "mtype\n",
       "Hyper        5\n",
       "Rural        3\n",
       "Suburban     5\n",
       "Urban       12\n",
       "Name: store_id, dtype: int64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.groupby(['mtype']).store_id.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-31T05:36:08.000841Z",
     "start_time": "2021-03-31T05:36:07.994027Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "type_dtsf\n",
       "DT       14\n",
       "FoodC     1\n",
       "Mall      2\n",
       "SF        8\n",
       "Name: store_id, dtype: int64"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.groupby(['type_dtsf']).store_id.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-31T05:36:08.012058Z",
     "start_time": "2021-03-31T05:36:08.002968Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "mtype     type_dtsf\n",
       "Hyper     SF           5\n",
       "Rural     DT           3\n",
       "Suburban  DT           3\n",
       "          FoodC        1\n",
       "          Mall         1\n",
       "Urban     DT           8\n",
       "          Mall         1\n",
       "          SF           3\n",
       "Name: store_id, dtype: int64"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# And now the cross distribution\n",
    "train.groupby(['mtype', 'type_dtsf']).store_id.nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "i.e. we only have a small subset of the fully crossed types, so we would be unwise to include interaction effects.\n",
    "\n",
    "We can also verify that the numerical features other than Seats, Parking_slots, and region vary across day (i.e. we don't have other \"constants\" that id the store). Looking at the non-proprietary features first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-31T05:36:08.040077Z",
     "start_time": "2021-03-31T05:36:08.018004Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>directionCode</th>\n",
       "      <th>totalActivitiesRefcircle</th>\n",
       "      <th>totalCustomersRefcircle</th>\n",
       "      <th>transactCount</th>\n",
       "      <th>totalRevenue</th>\n",
       "      <th>Seats</th>\n",
       "      <th>Parking_slots</th>\n",
       "      <th>region</th>\n",
       "      <th>deltaRevenue</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>store_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>(1, 0, 162)</th>\n",
       "      <td>0.0</td>\n",
       "      <td>5.634927e+05</td>\n",
       "      <td>108592.250775</td>\n",
       "      <td>404.471973</td>\n",
       "      <td>3.685018e+08</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>361.034792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(1, 13, 100)</th>\n",
       "      <td>0.0</td>\n",
       "      <td>2.305412e+06</td>\n",
       "      <td>372670.836028</td>\n",
       "      <td>382.307750</td>\n",
       "      <td>6.241073e+08</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>968.873756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(1, 15, 53)</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.380189e+05</td>\n",
       "      <td>21426.396304</td>\n",
       "      <td>561.939395</td>\n",
       "      <td>7.569915e+08</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>700.144777</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(1, 15, 106)</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.612085e+06</td>\n",
       "      <td>245438.890549</td>\n",
       "      <td>467.633724</td>\n",
       "      <td>6.965732e+08</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>739.081524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(1, 17, 94)</th>\n",
       "      <td>0.0</td>\n",
       "      <td>2.715052e+06</td>\n",
       "      <td>342394.173691</td>\n",
       "      <td>298.786929</td>\n",
       "      <td>5.935888e+08</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>980.683542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(1, 31, 120)</th>\n",
       "      <td>0.0</td>\n",
       "      <td>4.531734e+06</td>\n",
       "      <td>593939.605111</td>\n",
       "      <td>523.933389</td>\n",
       "      <td>1.110002e+09</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>903.367654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(1, 217, 188)</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.103942e+06</td>\n",
       "      <td>155178.546879</td>\n",
       "      <td>503.422539</td>\n",
       "      <td>8.521950e+08</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>902.187753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(2, 30, 92)</th>\n",
       "      <td>0.0</td>\n",
       "      <td>8.974360e+05</td>\n",
       "      <td>84107.484336</td>\n",
       "      <td>229.192855</td>\n",
       "      <td>4.413351e+08</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>704.261406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(2, 80, 64)</th>\n",
       "      <td>0.0</td>\n",
       "      <td>9.312435e+05</td>\n",
       "      <td>101112.191477</td>\n",
       "      <td>158.405068</td>\n",
       "      <td>2.766345e+08</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>746.661091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(3, 0, 158)</th>\n",
       "      <td>0.0</td>\n",
       "      <td>5.895244e+05</td>\n",
       "      <td>143381.282374</td>\n",
       "      <td>656.666936</td>\n",
       "      <td>8.459047e+07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>423.696269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(3, 0, 210)</th>\n",
       "      <td>0.0</td>\n",
       "      <td>7.137853e+04</td>\n",
       "      <td>7929.915770</td>\n",
       "      <td>796.306014</td>\n",
       "      <td>4.309439e+08</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>206.261231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(4, 0, 86)</th>\n",
       "      <td>0.0</td>\n",
       "      <td>2.200616e+05</td>\n",
       "      <td>71373.168288</td>\n",
       "      <td>743.841939</td>\n",
       "      <td>1.806808e+08</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>310.886804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(5, 21, 57)</th>\n",
       "      <td>0.0</td>\n",
       "      <td>2.840498e+06</td>\n",
       "      <td>244802.204865</td>\n",
       "      <td>179.465791</td>\n",
       "      <td>4.140692e+08</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>987.663772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(5, 1000, 126)</th>\n",
       "      <td>0.0</td>\n",
       "      <td>3.025550e+05</td>\n",
       "      <td>26623.740443</td>\n",
       "      <td>520.853119</td>\n",
       "      <td>7.577688e+08</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1648.402177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(5, 2500, 119)</th>\n",
       "      <td>0.0</td>\n",
       "      <td>3.695727e+05</td>\n",
       "      <td>32732.961491</td>\n",
       "      <td>576.073706</td>\n",
       "      <td>7.233546e+08</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1420.944525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(6, 37, 100)</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.971335e+05</td>\n",
       "      <td>35797.329524</td>\n",
       "      <td>776.911418</td>\n",
       "      <td>1.518113e+09</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>840.544476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(7, 61, 60)</th>\n",
       "      <td>0.0</td>\n",
       "      <td>3.987005e+05</td>\n",
       "      <td>72278.622004</td>\n",
       "      <td>629.630501</td>\n",
       "      <td>1.128599e+09</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1055.461548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(7, 600, 80)</th>\n",
       "      <td>NaN</td>\n",
       "      <td>4.203648e+05</td>\n",
       "      <td>63690.691168</td>\n",
       "      <td>538.218996</td>\n",
       "      <td>1.043642e+09</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>971.778833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(8, 0, 73)</th>\n",
       "      <td>0.0</td>\n",
       "      <td>2.670475e+05</td>\n",
       "      <td>85477.223391</td>\n",
       "      <td>411.394955</td>\n",
       "      <td>7.635660e+07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>390.304510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(8, 0, 105)</th>\n",
       "      <td>0.0</td>\n",
       "      <td>2.473683e+06</td>\n",
       "      <td>708143.989636</td>\n",
       "      <td>194.555796</td>\n",
       "      <td>9.763453e+07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>815.474445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(8, 0, 151)</th>\n",
       "      <td>0.0</td>\n",
       "      <td>3.100478e+06</td>\n",
       "      <td>780945.644543</td>\n",
       "      <td>585.659202</td>\n",
       "      <td>1.317464e+08</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>395401.280998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(9, 36, 52)</th>\n",
       "      <td>0.0</td>\n",
       "      <td>5.999830e+05</td>\n",
       "      <td>40735.323817</td>\n",
       "      <td>674.018514</td>\n",
       "      <td>1.651950e+09</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1141.100475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(9, 1500, 110)</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.317469e+05</td>\n",
       "      <td>9824.201342</td>\n",
       "      <td>210.114049</td>\n",
       "      <td>2.890509e+08</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1471.721371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(9, 1900, 548)</th>\n",
       "      <td>0.0</td>\n",
       "      <td>7.281447e+04</td>\n",
       "      <td>8706.716239</td>\n",
       "      <td>681.198291</td>\n",
       "      <td>8.638623e+08</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2456.226386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(10, 31, 131)</th>\n",
       "      <td>0.0</td>\n",
       "      <td>3.765243e+04</td>\n",
       "      <td>4537.241639</td>\n",
       "      <td>468.980411</td>\n",
       "      <td>5.254601e+08</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>715.209258</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                directionCode  totalActivitiesRefcircle  \\\n",
       "store_id                                                  \n",
       "(1, 0, 162)               0.0              5.634927e+05   \n",
       "(1, 13, 100)              0.0              2.305412e+06   \n",
       "(1, 15, 53)               0.0              1.380189e+05   \n",
       "(1, 15, 106)              0.0              1.612085e+06   \n",
       "(1, 17, 94)               0.0              2.715052e+06   \n",
       "(1, 31, 120)              0.0              4.531734e+06   \n",
       "(1, 217, 188)             0.0              1.103942e+06   \n",
       "(2, 30, 92)               0.0              8.974360e+05   \n",
       "(2, 80, 64)               0.0              9.312435e+05   \n",
       "(3, 0, 158)               0.0              5.895244e+05   \n",
       "(3, 0, 210)               0.0              7.137853e+04   \n",
       "(4, 0, 86)                0.0              2.200616e+05   \n",
       "(5, 21, 57)               0.0              2.840498e+06   \n",
       "(5, 1000, 126)            0.0              3.025550e+05   \n",
       "(5, 2500, 119)            0.0              3.695727e+05   \n",
       "(6, 37, 100)              0.0              1.971335e+05   \n",
       "(7, 61, 60)               0.0              3.987005e+05   \n",
       "(7, 600, 80)              NaN              4.203648e+05   \n",
       "(8, 0, 73)                0.0              2.670475e+05   \n",
       "(8, 0, 105)               0.0              2.473683e+06   \n",
       "(8, 0, 151)               0.0              3.100478e+06   \n",
       "(9, 36, 52)               0.0              5.999830e+05   \n",
       "(9, 1500, 110)            0.0              1.317469e+05   \n",
       "(9, 1900, 548)            0.0              7.281447e+04   \n",
       "(10, 31, 131)             0.0              3.765243e+04   \n",
       "\n",
       "                totalCustomersRefcircle  transactCount  totalRevenue  Seats  \\\n",
       "store_id                                                                      \n",
       "(1, 0, 162)               108592.250775     404.471973  3.685018e+08    0.0   \n",
       "(1, 13, 100)              372670.836028     382.307750  6.241073e+08    0.0   \n",
       "(1, 15, 53)                21426.396304     561.939395  7.569915e+08    0.0   \n",
       "(1, 15, 106)              245438.890549     467.633724  6.965732e+08    0.0   \n",
       "(1, 17, 94)               342394.173691     298.786929  5.935888e+08    0.0   \n",
       "(1, 31, 120)              593939.605111     523.933389  1.110002e+09    0.0   \n",
       "(1, 217, 188)             155178.546879     503.422539  8.521950e+08    0.0   \n",
       "(2, 30, 92)                84107.484336     229.192855  4.413351e+08    0.0   \n",
       "(2, 80, 64)               101112.191477     158.405068  2.766345e+08    0.0   \n",
       "(3, 0, 158)               143381.282374     656.666936  8.459047e+07    0.0   \n",
       "(3, 0, 210)                 7929.915770     796.306014  4.309439e+08    0.0   \n",
       "(4, 0, 86)                 71373.168288     743.841939  1.806808e+08    0.0   \n",
       "(5, 21, 57)               244802.204865     179.465791  4.140692e+08    0.0   \n",
       "(5, 1000, 126)             26623.740443     520.853119  7.577688e+08    0.0   \n",
       "(5, 2500, 119)             32732.961491     576.073706  7.233546e+08    0.0   \n",
       "(6, 37, 100)               35797.329524     776.911418  1.518113e+09    0.0   \n",
       "(7, 61, 60)                72278.622004     629.630501  1.128599e+09    0.0   \n",
       "(7, 600, 80)               63690.691168     538.218996  1.043642e+09    0.0   \n",
       "(8, 0, 73)                 85477.223391     411.394955  7.635660e+07    0.0   \n",
       "(8, 0, 105)               708143.989636     194.555796  9.763453e+07    0.0   \n",
       "(8, 0, 151)               780945.644543     585.659202  1.317464e+08    0.0   \n",
       "(9, 36, 52)                40735.323817     674.018514  1.651950e+09    0.0   \n",
       "(9, 1500, 110)              9824.201342     210.114049  2.890509e+08    0.0   \n",
       "(9, 1900, 548)              8706.716239     681.198291  8.638623e+08    0.0   \n",
       "(10, 31, 131)               4537.241639     468.980411  5.254601e+08    0.0   \n",
       "\n",
       "                Parking_slots  region   deltaRevenue  \n",
       "store_id                                              \n",
       "(1, 0, 162)               0.0     0.0     361.034792  \n",
       "(1, 13, 100)              0.0     0.0     968.873756  \n",
       "(1, 15, 53)               0.0     0.0     700.144777  \n",
       "(1, 15, 106)              0.0     0.0     739.081524  \n",
       "(1, 17, 94)               0.0     0.0     980.683542  \n",
       "(1, 31, 120)              0.0     0.0     903.367654  \n",
       "(1, 217, 188)             0.0     0.0     902.187753  \n",
       "(2, 30, 92)               0.0     0.0     704.261406  \n",
       "(2, 80, 64)               0.0     0.0     746.661091  \n",
       "(3, 0, 158)               0.0     0.0     423.696269  \n",
       "(3, 0, 210)               0.0     0.0     206.261231  \n",
       "(4, 0, 86)                0.0     0.0     310.886804  \n",
       "(5, 21, 57)               0.0     0.0     987.663772  \n",
       "(5, 1000, 126)            0.0     0.0    1648.402177  \n",
       "(5, 2500, 119)            0.0     0.0    1420.944525  \n",
       "(6, 37, 100)              0.0     0.0     840.544476  \n",
       "(7, 61, 60)               0.0     0.0    1055.461548  \n",
       "(7, 600, 80)              0.0     0.0     971.778833  \n",
       "(8, 0, 73)                0.0     0.0     390.304510  \n",
       "(8, 0, 105)               0.0     0.0     815.474445  \n",
       "(8, 0, 151)               0.0     0.0  395401.280998  \n",
       "(9, 36, 52)               0.0     0.0    1141.100475  \n",
       "(9, 1500, 110)            0.0     0.0    1471.721371  \n",
       "(9, 1900, 548)            0.0     0.0    2456.226386  \n",
       "(10, 31, 131)             0.0     0.0     715.209258  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_to_map = [\n",
    "    'directionCode',\n",
    "    'totalActivitiesRefcircle',\n",
    "    'totalCustomersRefcircle',\n",
    "    'transactCount',\n",
    "    'totalRevenue',\n",
    "    'Seats',\n",
    "    'Parking_slots',\n",
    "    'region',\n",
    "    'deltaRevenue'\n",
    "]\n",
    "\n",
    "train.groupby('store_id')[features_to_map].var()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now looking at the proprietary (i.e. `r_*`) features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-31T05:36:08.070558Z",
     "start_time": "2021-03-31T05:36:08.043435Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>r_11</th>\n",
       "      <th>r_12</th>\n",
       "      <th>r_13</th>\n",
       "      <th>r_14</th>\n",
       "      <th>r_1e</th>\n",
       "      <th>r_21</th>\n",
       "      <th>r_22</th>\n",
       "      <th>r_23</th>\n",
       "      <th>r_24</th>\n",
       "      <th>r_2e</th>\n",
       "      <th>r_31</th>\n",
       "      <th>r_32</th>\n",
       "      <th>r_33</th>\n",
       "      <th>r_34</th>\n",
       "      <th>r_3e</th>\n",
       "      <th>r_41</th>\n",
       "      <th>r_42</th>\n",
       "      <th>r_43</th>\n",
       "      <th>r_44</th>\n",
       "      <th>r_4e</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>store_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>(1, 0, 162)</th>\n",
       "      <td>0.001001</td>\n",
       "      <td>0.001155</td>\n",
       "      <td>0.005395</td>\n",
       "      <td>0.006296</td>\n",
       "      <td>0.000040</td>\n",
       "      <td>0.001275</td>\n",
       "      <td>0.000335</td>\n",
       "      <td>0.002896</td>\n",
       "      <td>0.006575</td>\n",
       "      <td>0.000213</td>\n",
       "      <td>0.008937</td>\n",
       "      <td>0.003209</td>\n",
       "      <td>0.001177</td>\n",
       "      <td>0.002545</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>0.005419</td>\n",
       "      <td>0.006996</td>\n",
       "      <td>0.001649</td>\n",
       "      <td>0.000721</td>\n",
       "      <td>0.000007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(1, 13, 100)</th>\n",
       "      <td>0.000326</td>\n",
       "      <td>0.001198</td>\n",
       "      <td>0.003424</td>\n",
       "      <td>0.001476</td>\n",
       "      <td>0.000124</td>\n",
       "      <td>0.000989</td>\n",
       "      <td>0.000213</td>\n",
       "      <td>0.001166</td>\n",
       "      <td>0.006245</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.003961</td>\n",
       "      <td>0.000981</td>\n",
       "      <td>0.000428</td>\n",
       "      <td>0.001148</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.001328</td>\n",
       "      <td>0.004548</td>\n",
       "      <td>0.001545</td>\n",
       "      <td>0.001195</td>\n",
       "      <td>0.000013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(1, 15, 53)</th>\n",
       "      <td>0.018857</td>\n",
       "      <td>0.036865</td>\n",
       "      <td>0.054026</td>\n",
       "      <td>0.033617</td>\n",
       "      <td>0.001750</td>\n",
       "      <td>0.058517</td>\n",
       "      <td>0.016003</td>\n",
       "      <td>0.004598</td>\n",
       "      <td>0.009365</td>\n",
       "      <td>0.000017</td>\n",
       "      <td>0.077743</td>\n",
       "      <td>0.004018</td>\n",
       "      <td>0.014562</td>\n",
       "      <td>0.036114</td>\n",
       "      <td>0.000145</td>\n",
       "      <td>0.013988</td>\n",
       "      <td>0.005709</td>\n",
       "      <td>0.040146</td>\n",
       "      <td>0.014747</td>\n",
       "      <td>0.002168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(1, 15, 106)</th>\n",
       "      <td>0.014679</td>\n",
       "      <td>0.003509</td>\n",
       "      <td>0.001902</td>\n",
       "      <td>0.007179</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.004042</td>\n",
       "      <td>0.012226</td>\n",
       "      <td>0.002645</td>\n",
       "      <td>0.049495</td>\n",
       "      <td>0.000076</td>\n",
       "      <td>0.002488</td>\n",
       "      <td>0.001836</td>\n",
       "      <td>0.016048</td>\n",
       "      <td>0.003768</td>\n",
       "      <td>0.000796</td>\n",
       "      <td>0.008611</td>\n",
       "      <td>0.051729</td>\n",
       "      <td>0.004822</td>\n",
       "      <td>0.011941</td>\n",
       "      <td>0.000052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(1, 17, 94)</th>\n",
       "      <td>0.001104</td>\n",
       "      <td>0.001658</td>\n",
       "      <td>0.005145</td>\n",
       "      <td>0.000689</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.001168</td>\n",
       "      <td>0.000337</td>\n",
       "      <td>0.001899</td>\n",
       "      <td>0.002953</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.004227</td>\n",
       "      <td>0.002507</td>\n",
       "      <td>0.001110</td>\n",
       "      <td>0.000548</td>\n",
       "      <td>0.000025</td>\n",
       "      <td>0.000855</td>\n",
       "      <td>0.002835</td>\n",
       "      <td>0.001039</td>\n",
       "      <td>0.000358</td>\n",
       "      <td>0.000040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(1, 31, 120)</th>\n",
       "      <td>0.000303</td>\n",
       "      <td>0.000599</td>\n",
       "      <td>0.001267</td>\n",
       "      <td>0.000896</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>0.000476</td>\n",
       "      <td>0.000566</td>\n",
       "      <td>0.000584</td>\n",
       "      <td>0.000846</td>\n",
       "      <td>0.000058</td>\n",
       "      <td>0.001980</td>\n",
       "      <td>0.000759</td>\n",
       "      <td>0.001248</td>\n",
       "      <td>0.000808</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.000972</td>\n",
       "      <td>0.001289</td>\n",
       "      <td>0.000913</td>\n",
       "      <td>0.000437</td>\n",
       "      <td>0.000001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(1, 217, 188)</th>\n",
       "      <td>0.000855</td>\n",
       "      <td>0.001983</td>\n",
       "      <td>0.008055</td>\n",
       "      <td>0.002691</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>0.001616</td>\n",
       "      <td>0.000286</td>\n",
       "      <td>0.003902</td>\n",
       "      <td>0.005852</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.006768</td>\n",
       "      <td>0.003560</td>\n",
       "      <td>0.000740</td>\n",
       "      <td>0.002100</td>\n",
       "      <td>0.000038</td>\n",
       "      <td>0.002889</td>\n",
       "      <td>0.010076</td>\n",
       "      <td>0.002375</td>\n",
       "      <td>0.000305</td>\n",
       "      <td>0.000235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(2, 30, 92)</th>\n",
       "      <td>0.005116</td>\n",
       "      <td>0.001412</td>\n",
       "      <td>0.002080</td>\n",
       "      <td>0.027771</td>\n",
       "      <td>0.001104</td>\n",
       "      <td>0.002421</td>\n",
       "      <td>0.002304</td>\n",
       "      <td>0.002020</td>\n",
       "      <td>0.049324</td>\n",
       "      <td>0.000134</td>\n",
       "      <td>0.002780</td>\n",
       "      <td>0.001863</td>\n",
       "      <td>0.003692</td>\n",
       "      <td>0.025915</td>\n",
       "      <td>0.000051</td>\n",
       "      <td>0.031344</td>\n",
       "      <td>0.032874</td>\n",
       "      <td>0.023101</td>\n",
       "      <td>0.026566</td>\n",
       "      <td>0.001406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(2, 80, 64)</th>\n",
       "      <td>0.008184</td>\n",
       "      <td>0.047202</td>\n",
       "      <td>0.049349</td>\n",
       "      <td>0.004986</td>\n",
       "      <td>0.000074</td>\n",
       "      <td>0.050552</td>\n",
       "      <td>0.001985</td>\n",
       "      <td>0.001804</td>\n",
       "      <td>0.009868</td>\n",
       "      <td>0.000979</td>\n",
       "      <td>0.054484</td>\n",
       "      <td>0.002554</td>\n",
       "      <td>0.001345</td>\n",
       "      <td>0.001578</td>\n",
       "      <td>0.000136</td>\n",
       "      <td>0.004089</td>\n",
       "      <td>0.004852</td>\n",
       "      <td>0.001259</td>\n",
       "      <td>0.002358</td>\n",
       "      <td>0.000006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(3, 0, 158)</th>\n",
       "      <td>0.002036</td>\n",
       "      <td>0.020336</td>\n",
       "      <td>0.051696</td>\n",
       "      <td>0.029490</td>\n",
       "      <td>0.000065</td>\n",
       "      <td>0.012086</td>\n",
       "      <td>0.000596</td>\n",
       "      <td>0.016003</td>\n",
       "      <td>0.014425</td>\n",
       "      <td>0.000018</td>\n",
       "      <td>0.086772</td>\n",
       "      <td>0.015823</td>\n",
       "      <td>0.003904</td>\n",
       "      <td>0.004046</td>\n",
       "      <td>0.000401</td>\n",
       "      <td>0.024982</td>\n",
       "      <td>0.028207</td>\n",
       "      <td>0.002817</td>\n",
       "      <td>0.000345</td>\n",
       "      <td>0.001197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(3, 0, 210)</th>\n",
       "      <td>0.003693</td>\n",
       "      <td>0.021540</td>\n",
       "      <td>0.076003</td>\n",
       "      <td>0.007684</td>\n",
       "      <td>0.000386</td>\n",
       "      <td>0.024904</td>\n",
       "      <td>0.003513</td>\n",
       "      <td>0.032142</td>\n",
       "      <td>0.004956</td>\n",
       "      <td>0.000536</td>\n",
       "      <td>0.087779</td>\n",
       "      <td>0.020193</td>\n",
       "      <td>0.006206</td>\n",
       "      <td>0.001467</td>\n",
       "      <td>0.001412</td>\n",
       "      <td>0.008223</td>\n",
       "      <td>0.002444</td>\n",
       "      <td>0.000915</td>\n",
       "      <td>0.001033</td>\n",
       "      <td>0.000453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(4, 0, 86)</th>\n",
       "      <td>0.002480</td>\n",
       "      <td>0.015099</td>\n",
       "      <td>0.009374</td>\n",
       "      <td>0.000375</td>\n",
       "      <td>0.000632</td>\n",
       "      <td>0.035913</td>\n",
       "      <td>0.002898</td>\n",
       "      <td>0.041203</td>\n",
       "      <td>0.082673</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>0.007247</td>\n",
       "      <td>0.026841</td>\n",
       "      <td>0.000690</td>\n",
       "      <td>0.007364</td>\n",
       "      <td>0.000029</td>\n",
       "      <td>0.000519</td>\n",
       "      <td>0.084260</td>\n",
       "      <td>0.009226</td>\n",
       "      <td>0.003763</td>\n",
       "      <td>0.000323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(5, 21, 57)</th>\n",
       "      <td>0.002376</td>\n",
       "      <td>0.001027</td>\n",
       "      <td>0.023984</td>\n",
       "      <td>0.004903</td>\n",
       "      <td>0.000576</td>\n",
       "      <td>0.001101</td>\n",
       "      <td>0.001126</td>\n",
       "      <td>0.022717</td>\n",
       "      <td>0.007839</td>\n",
       "      <td>0.000434</td>\n",
       "      <td>0.016440</td>\n",
       "      <td>0.030907</td>\n",
       "      <td>0.003527</td>\n",
       "      <td>0.000345</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.007361</td>\n",
       "      <td>0.008838</td>\n",
       "      <td>0.000493</td>\n",
       "      <td>0.002446</td>\n",
       "      <td>0.000006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(5, 1000, 126)</th>\n",
       "      <td>0.000220</td>\n",
       "      <td>0.002800</td>\n",
       "      <td>0.050309</td>\n",
       "      <td>0.006140</td>\n",
       "      <td>0.000012</td>\n",
       "      <td>0.004744</td>\n",
       "      <td>0.001123</td>\n",
       "      <td>0.031696</td>\n",
       "      <td>0.011711</td>\n",
       "      <td>0.000464</td>\n",
       "      <td>0.066494</td>\n",
       "      <td>0.008137</td>\n",
       "      <td>0.003011</td>\n",
       "      <td>0.012419</td>\n",
       "      <td>0.000035</td>\n",
       "      <td>0.004625</td>\n",
       "      <td>0.013258</td>\n",
       "      <td>0.017615</td>\n",
       "      <td>0.001082</td>\n",
       "      <td>0.000008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(5, 2500, 119)</th>\n",
       "      <td>0.000456</td>\n",
       "      <td>0.007042</td>\n",
       "      <td>0.007493</td>\n",
       "      <td>0.000743</td>\n",
       "      <td>0.000340</td>\n",
       "      <td>0.004144</td>\n",
       "      <td>0.003087</td>\n",
       "      <td>0.007133</td>\n",
       "      <td>0.028880</td>\n",
       "      <td>0.000187</td>\n",
       "      <td>0.017415</td>\n",
       "      <td>0.008455</td>\n",
       "      <td>0.000867</td>\n",
       "      <td>0.002039</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>0.000512</td>\n",
       "      <td>0.014205</td>\n",
       "      <td>0.002633</td>\n",
       "      <td>0.000790</td>\n",
       "      <td>0.000007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(6, 37, 100)</th>\n",
       "      <td>0.000944</td>\n",
       "      <td>0.003470</td>\n",
       "      <td>0.010251</td>\n",
       "      <td>0.017712</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.010972</td>\n",
       "      <td>0.005865</td>\n",
       "      <td>0.024463</td>\n",
       "      <td>0.054718</td>\n",
       "      <td>0.001147</td>\n",
       "      <td>0.025179</td>\n",
       "      <td>0.041829</td>\n",
       "      <td>0.000649</td>\n",
       "      <td>0.017143</td>\n",
       "      <td>0.000028</td>\n",
       "      <td>0.023111</td>\n",
       "      <td>0.071421</td>\n",
       "      <td>0.020539</td>\n",
       "      <td>0.004627</td>\n",
       "      <td>0.000419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(7, 61, 60)</th>\n",
       "      <td>0.000554</td>\n",
       "      <td>0.000648</td>\n",
       "      <td>0.003946</td>\n",
       "      <td>0.001667</td>\n",
       "      <td>0.000381</td>\n",
       "      <td>0.000685</td>\n",
       "      <td>0.000731</td>\n",
       "      <td>0.002915</td>\n",
       "      <td>0.003654</td>\n",
       "      <td>0.000030</td>\n",
       "      <td>0.007084</td>\n",
       "      <td>0.002902</td>\n",
       "      <td>0.001137</td>\n",
       "      <td>0.001326</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.001653</td>\n",
       "      <td>0.003533</td>\n",
       "      <td>0.001404</td>\n",
       "      <td>0.001272</td>\n",
       "      <td>0.000030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(7, 600, 80)</th>\n",
       "      <td>0.000573</td>\n",
       "      <td>0.001094</td>\n",
       "      <td>0.003853</td>\n",
       "      <td>0.001388</td>\n",
       "      <td>0.000044</td>\n",
       "      <td>0.001333</td>\n",
       "      <td>0.000718</td>\n",
       "      <td>0.000724</td>\n",
       "      <td>0.007940</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.003808</td>\n",
       "      <td>0.001197</td>\n",
       "      <td>0.001966</td>\n",
       "      <td>0.003698</td>\n",
       "      <td>0.000028</td>\n",
       "      <td>0.002019</td>\n",
       "      <td>0.007109</td>\n",
       "      <td>0.003202</td>\n",
       "      <td>0.000676</td>\n",
       "      <td>0.000329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(8, 0, 73)</th>\n",
       "      <td>0.021683</td>\n",
       "      <td>0.017653</td>\n",
       "      <td>0.013068</td>\n",
       "      <td>0.028946</td>\n",
       "      <td>0.000140</td>\n",
       "      <td>0.021878</td>\n",
       "      <td>0.020883</td>\n",
       "      <td>0.012267</td>\n",
       "      <td>0.067893</td>\n",
       "      <td>0.000048</td>\n",
       "      <td>0.013585</td>\n",
       "      <td>0.017517</td>\n",
       "      <td>0.016976</td>\n",
       "      <td>0.003188</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>0.034626</td>\n",
       "      <td>0.054418</td>\n",
       "      <td>0.001188</td>\n",
       "      <td>0.014526</td>\n",
       "      <td>0.000020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(8, 0, 105)</th>\n",
       "      <td>0.022364</td>\n",
       "      <td>0.004390</td>\n",
       "      <td>0.023280</td>\n",
       "      <td>0.005144</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.006334</td>\n",
       "      <td>0.021823</td>\n",
       "      <td>0.002233</td>\n",
       "      <td>0.010787</td>\n",
       "      <td>0.000060</td>\n",
       "      <td>0.027212</td>\n",
       "      <td>0.002596</td>\n",
       "      <td>0.018229</td>\n",
       "      <td>0.019740</td>\n",
       "      <td>0.000167</td>\n",
       "      <td>0.005490</td>\n",
       "      <td>0.010785</td>\n",
       "      <td>0.018474</td>\n",
       "      <td>0.021958</td>\n",
       "      <td>0.000002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(8, 0, 151)</th>\n",
       "      <td>0.020562</td>\n",
       "      <td>0.002839</td>\n",
       "      <td>0.013191</td>\n",
       "      <td>0.003433</td>\n",
       "      <td>0.000018</td>\n",
       "      <td>0.001090</td>\n",
       "      <td>0.021098</td>\n",
       "      <td>0.004956</td>\n",
       "      <td>0.007931</td>\n",
       "      <td>0.000536</td>\n",
       "      <td>0.003596</td>\n",
       "      <td>0.007324</td>\n",
       "      <td>0.020594</td>\n",
       "      <td>0.003932</td>\n",
       "      <td>0.000015</td>\n",
       "      <td>0.001161</td>\n",
       "      <td>0.013546</td>\n",
       "      <td>0.005641</td>\n",
       "      <td>0.022011</td>\n",
       "      <td>0.000002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(9, 36, 52)</th>\n",
       "      <td>0.004710</td>\n",
       "      <td>0.003545</td>\n",
       "      <td>0.014630</td>\n",
       "      <td>0.006629</td>\n",
       "      <td>0.000201</td>\n",
       "      <td>0.003910</td>\n",
       "      <td>0.000526</td>\n",
       "      <td>0.004022</td>\n",
       "      <td>0.014245</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.017214</td>\n",
       "      <td>0.006871</td>\n",
       "      <td>0.000464</td>\n",
       "      <td>0.006251</td>\n",
       "      <td>0.000016</td>\n",
       "      <td>0.003634</td>\n",
       "      <td>0.009702</td>\n",
       "      <td>0.006668</td>\n",
       "      <td>0.000382</td>\n",
       "      <td>0.000425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(9, 1500, 110)</th>\n",
       "      <td>0.007878</td>\n",
       "      <td>0.017757</td>\n",
       "      <td>0.070517</td>\n",
       "      <td>0.005593</td>\n",
       "      <td>0.000050</td>\n",
       "      <td>0.043891</td>\n",
       "      <td>0.001412</td>\n",
       "      <td>0.011426</td>\n",
       "      <td>0.015936</td>\n",
       "      <td>0.000984</td>\n",
       "      <td>0.056605</td>\n",
       "      <td>0.009766</td>\n",
       "      <td>0.005243</td>\n",
       "      <td>0.005868</td>\n",
       "      <td>0.000441</td>\n",
       "      <td>0.001347</td>\n",
       "      <td>0.005446</td>\n",
       "      <td>0.010247</td>\n",
       "      <td>0.001030</td>\n",
       "      <td>0.000022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(9, 1900, 548)</th>\n",
       "      <td>0.005931</td>\n",
       "      <td>0.001759</td>\n",
       "      <td>0.053802</td>\n",
       "      <td>0.067724</td>\n",
       "      <td>0.000042</td>\n",
       "      <td>0.001744</td>\n",
       "      <td>0.001130</td>\n",
       "      <td>0.011070</td>\n",
       "      <td>0.054162</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.045636</td>\n",
       "      <td>0.007829</td>\n",
       "      <td>0.003008</td>\n",
       "      <td>0.009300</td>\n",
       "      <td>0.000526</td>\n",
       "      <td>0.087020</td>\n",
       "      <td>0.042630</td>\n",
       "      <td>0.007150</td>\n",
       "      <td>0.001019</td>\n",
       "      <td>0.001038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(10, 31, 131)</th>\n",
       "      <td>0.002451</td>\n",
       "      <td>0.037090</td>\n",
       "      <td>0.008690</td>\n",
       "      <td>0.015400</td>\n",
       "      <td>0.002117</td>\n",
       "      <td>0.035939</td>\n",
       "      <td>0.009822</td>\n",
       "      <td>0.009041</td>\n",
       "      <td>0.086649</td>\n",
       "      <td>0.000580</td>\n",
       "      <td>0.027907</td>\n",
       "      <td>0.005515</td>\n",
       "      <td>0.000680</td>\n",
       "      <td>0.013954</td>\n",
       "      <td>0.000045</td>\n",
       "      <td>0.014211</td>\n",
       "      <td>0.099599</td>\n",
       "      <td>0.027652</td>\n",
       "      <td>0.005101</td>\n",
       "      <td>0.001657</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    r_11      r_12      r_13      r_14      r_1e      r_21  \\\n",
       "store_id                                                                     \n",
       "(1, 0, 162)     0.001001  0.001155  0.005395  0.006296  0.000040  0.001275   \n",
       "(1, 13, 100)    0.000326  0.001198  0.003424  0.001476  0.000124  0.000989   \n",
       "(1, 15, 53)     0.018857  0.036865  0.054026  0.033617  0.001750  0.058517   \n",
       "(1, 15, 106)    0.014679  0.003509  0.001902  0.007179  0.000003  0.004042   \n",
       "(1, 17, 94)     0.001104  0.001658  0.005145  0.000689  0.000002  0.001168   \n",
       "(1, 31, 120)    0.000303  0.000599  0.001267  0.000896  0.000005  0.000476   \n",
       "(1, 217, 188)   0.000855  0.001983  0.008055  0.002691  0.000011  0.001616   \n",
       "(2, 30, 92)     0.005116  0.001412  0.002080  0.027771  0.001104  0.002421   \n",
       "(2, 80, 64)     0.008184  0.047202  0.049349  0.004986  0.000074  0.050552   \n",
       "(3, 0, 158)     0.002036  0.020336  0.051696  0.029490  0.000065  0.012086   \n",
       "(3, 0, 210)     0.003693  0.021540  0.076003  0.007684  0.000386  0.024904   \n",
       "(4, 0, 86)      0.002480  0.015099  0.009374  0.000375  0.000632  0.035913   \n",
       "(5, 21, 57)     0.002376  0.001027  0.023984  0.004903  0.000576  0.001101   \n",
       "(5, 1000, 126)  0.000220  0.002800  0.050309  0.006140  0.000012  0.004744   \n",
       "(5, 2500, 119)  0.000456  0.007042  0.007493  0.000743  0.000340  0.004144   \n",
       "(6, 37, 100)    0.000944  0.003470  0.010251  0.017712  0.001000  0.010972   \n",
       "(7, 61, 60)     0.000554  0.000648  0.003946  0.001667  0.000381  0.000685   \n",
       "(7, 600, 80)    0.000573  0.001094  0.003853  0.001388  0.000044  0.001333   \n",
       "(8, 0, 73)      0.021683  0.017653  0.013068  0.028946  0.000140  0.021878   \n",
       "(8, 0, 105)     0.022364  0.004390  0.023280  0.005144  0.000002  0.006334   \n",
       "(8, 0, 151)     0.020562  0.002839  0.013191  0.003433  0.000018  0.001090   \n",
       "(9, 36, 52)     0.004710  0.003545  0.014630  0.006629  0.000201  0.003910   \n",
       "(9, 1500, 110)  0.007878  0.017757  0.070517  0.005593  0.000050  0.043891   \n",
       "(9, 1900, 548)  0.005931  0.001759  0.053802  0.067724  0.000042  0.001744   \n",
       "(10, 31, 131)   0.002451  0.037090  0.008690  0.015400  0.002117  0.035939   \n",
       "\n",
       "                    r_22      r_23      r_24      r_2e      r_31      r_32  \\\n",
       "store_id                                                                     \n",
       "(1, 0, 162)     0.000335  0.002896  0.006575  0.000213  0.008937  0.003209   \n",
       "(1, 13, 100)    0.000213  0.001166  0.006245  0.000003  0.003961  0.000981   \n",
       "(1, 15, 53)     0.016003  0.004598  0.009365  0.000017  0.077743  0.004018   \n",
       "(1, 15, 106)    0.012226  0.002645  0.049495  0.000076  0.002488  0.001836   \n",
       "(1, 17, 94)     0.000337  0.001899  0.002953  0.000001  0.004227  0.002507   \n",
       "(1, 31, 120)    0.000566  0.000584  0.000846  0.000058  0.001980  0.000759   \n",
       "(1, 217, 188)   0.000286  0.003902  0.005852  0.000002  0.006768  0.003560   \n",
       "(2, 30, 92)     0.002304  0.002020  0.049324  0.000134  0.002780  0.001863   \n",
       "(2, 80, 64)     0.001985  0.001804  0.009868  0.000979  0.054484  0.002554   \n",
       "(3, 0, 158)     0.000596  0.016003  0.014425  0.000018  0.086772  0.015823   \n",
       "(3, 0, 210)     0.003513  0.032142  0.004956  0.000536  0.087779  0.020193   \n",
       "(4, 0, 86)      0.002898  0.041203  0.082673  0.000200  0.007247  0.026841   \n",
       "(5, 21, 57)     0.001126  0.022717  0.007839  0.000434  0.016440  0.030907   \n",
       "(5, 1000, 126)  0.001123  0.031696  0.011711  0.000464  0.066494  0.008137   \n",
       "(5, 2500, 119)  0.003087  0.007133  0.028880  0.000187  0.017415  0.008455   \n",
       "(6, 37, 100)    0.005865  0.024463  0.054718  0.001147  0.025179  0.041829   \n",
       "(7, 61, 60)     0.000731  0.002915  0.003654  0.000030  0.007084  0.002902   \n",
       "(7, 600, 80)    0.000718  0.000724  0.007940  0.000004  0.003808  0.001197   \n",
       "(8, 0, 73)      0.020883  0.012267  0.067893  0.000048  0.013585  0.017517   \n",
       "(8, 0, 105)     0.021823  0.002233  0.010787  0.000060  0.027212  0.002596   \n",
       "(8, 0, 151)     0.021098  0.004956  0.007931  0.000536  0.003596  0.007324   \n",
       "(9, 36, 52)     0.000526  0.004022  0.014245  0.000014  0.017214  0.006871   \n",
       "(9, 1500, 110)  0.001412  0.011426  0.015936  0.000984  0.056605  0.009766   \n",
       "(9, 1900, 548)  0.001130  0.011070  0.054162  0.000006  0.045636  0.007829   \n",
       "(10, 31, 131)   0.009822  0.009041  0.086649  0.000580  0.027907  0.005515   \n",
       "\n",
       "                    r_33      r_34      r_3e      r_41      r_42      r_43  \\\n",
       "store_id                                                                     \n",
       "(1, 0, 162)     0.001177  0.002545  0.000011  0.005419  0.006996  0.001649   \n",
       "(1, 13, 100)    0.000428  0.001148  0.000002  0.001328  0.004548  0.001545   \n",
       "(1, 15, 53)     0.014562  0.036114  0.000145  0.013988  0.005709  0.040146   \n",
       "(1, 15, 106)    0.016048  0.003768  0.000796  0.008611  0.051729  0.004822   \n",
       "(1, 17, 94)     0.001110  0.000548  0.000025  0.000855  0.002835  0.001039   \n",
       "(1, 31, 120)    0.001248  0.000808  0.000003  0.000972  0.001289  0.000913   \n",
       "(1, 217, 188)   0.000740  0.002100  0.000038  0.002889  0.010076  0.002375   \n",
       "(2, 30, 92)     0.003692  0.025915  0.000051  0.031344  0.032874  0.023101   \n",
       "(2, 80, 64)     0.001345  0.001578  0.000136  0.004089  0.004852  0.001259   \n",
       "(3, 0, 158)     0.003904  0.004046  0.000401  0.024982  0.028207  0.002817   \n",
       "(3, 0, 210)     0.006206  0.001467  0.001412  0.008223  0.002444  0.000915   \n",
       "(4, 0, 86)      0.000690  0.007364  0.000029  0.000519  0.084260  0.009226   \n",
       "(5, 21, 57)     0.003527  0.000345  0.000006  0.007361  0.008838  0.000493   \n",
       "(5, 1000, 126)  0.003011  0.012419  0.000035  0.004625  0.013258  0.017615   \n",
       "(5, 2500, 119)  0.000867  0.002039  0.000007  0.000512  0.014205  0.002633   \n",
       "(6, 37, 100)    0.000649  0.017143  0.000028  0.023111  0.071421  0.020539   \n",
       "(7, 61, 60)     0.001137  0.001326  0.000010  0.001653  0.003533  0.001404   \n",
       "(7, 600, 80)    0.001966  0.003698  0.000028  0.002019  0.007109  0.003202   \n",
       "(8, 0, 73)      0.016976  0.003188  0.000007  0.034626  0.054418  0.001188   \n",
       "(8, 0, 105)     0.018229  0.019740  0.000167  0.005490  0.010785  0.018474   \n",
       "(8, 0, 151)     0.020594  0.003932  0.000015  0.001161  0.013546  0.005641   \n",
       "(9, 36, 52)     0.000464  0.006251  0.000016  0.003634  0.009702  0.006668   \n",
       "(9, 1500, 110)  0.005243  0.005868  0.000441  0.001347  0.005446  0.010247   \n",
       "(9, 1900, 548)  0.003008  0.009300  0.000526  0.087020  0.042630  0.007150   \n",
       "(10, 31, 131)   0.000680  0.013954  0.000045  0.014211  0.099599  0.027652   \n",
       "\n",
       "                    r_44      r_4e  \n",
       "store_id                            \n",
       "(1, 0, 162)     0.000721  0.000007  \n",
       "(1, 13, 100)    0.001195  0.000013  \n",
       "(1, 15, 53)     0.014747  0.002168  \n",
       "(1, 15, 106)    0.011941  0.000052  \n",
       "(1, 17, 94)     0.000358  0.000040  \n",
       "(1, 31, 120)    0.000437  0.000001  \n",
       "(1, 217, 188)   0.000305  0.000235  \n",
       "(2, 30, 92)     0.026566  0.001406  \n",
       "(2, 80, 64)     0.002358  0.000006  \n",
       "(3, 0, 158)     0.000345  0.001197  \n",
       "(3, 0, 210)     0.001033  0.000453  \n",
       "(4, 0, 86)      0.003763  0.000323  \n",
       "(5, 21, 57)     0.002446  0.000006  \n",
       "(5, 1000, 126)  0.001082  0.000008  \n",
       "(5, 2500, 119)  0.000790  0.000007  \n",
       "(6, 37, 100)    0.004627  0.000419  \n",
       "(7, 61, 60)     0.001272  0.000030  \n",
       "(7, 600, 80)    0.000676  0.000329  \n",
       "(8, 0, 73)      0.014526  0.000020  \n",
       "(8, 0, 105)     0.021958  0.000002  \n",
       "(8, 0, 151)     0.022011  0.000002  \n",
       "(9, 36, 52)     0.000382  0.000425  \n",
       "(9, 1500, 110)  0.001030  0.000022  \n",
       "(9, 1900, 548)  0.001019  0.001038  \n",
       "(10, 31, 131)   0.005101  0.001657  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.groupby('store_id')[[c for c in train.columns if 'r_' in c]].var()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Bottom line:**\n",
    "\n",
    "Market type and store type are constant across `store_id`, but the other features vary from day-to-day within a particular store.\n",
    "\n",
    "We will need to model variables that ID a store `(region, parking, seats, market type, store type)` to get information relevant about the store, and the other information will be time varying. We only have 25 independent points for modeling these 5 variables, so we will need a pretty simple model!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $\\S$1.5 EDA: Looking at variables and their relationship to the output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's steer clear of the novel features for the moment. We will test them for predictive power separately, but let's start with the interpretable features and their correlation with the desired output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-03-31T05:36:05.699Z"
    }
   },
   "outputs": [],
   "source": [
    "sns.pairplot(data=train, \n",
    "             x_vars=features_to_map,\n",
    "             y_vars='deltaRevenue')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, we have a clear outlier in our dataset! We should see how big a problem this is (it is likely a data error or a extreme event that caused revenue to go up for unexpected reasons)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-03-31T05:36:05.703Z"
    }
   },
   "outputs": [],
   "source": [
    "train.sort_values('deltaRevenue', ascending=False).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, let's drop the one obvious outlier (that is almost certainly a data error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-03-31T05:36:05.705Z"
    }
   },
   "outputs": [],
   "source": [
    "train = train[train.deltaRevenue < 300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-03-31T05:36:05.707Z"
    }
   },
   "outputs": [],
   "source": [
    "sns.pairplot(data=train, \n",
    "             x_vars=features_to_map,\n",
    "             y_vars='deltaRevenue')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A few callouts here:\n",
    "\n",
    "1. Both `transactCount` and `totalRevenue` are positively correlated with `deltaRevenue`. Basically the more your store does in revenue, the more we expect the `deltaRevenue` to be\n",
    "2. Depending on whether `transactCount` and `totalRevenue` are of the _existing_ store or the _new_ store, it may or may not be fair to include it. In both cases, we don't actually have the measurement of transations of revenue until the day in question, so we wouldn't be able to put measured values into our model!\n",
    "  - If it is for the transactions of the _existing store_, we probably should not include it. We have measurements of the typical revenue of the store _after_ the new store was put in, but we don't know what it was before. The thing that we are trying to measure is the effect of putting the store in!\n",
    "    - We could do something a more complicated: use `deltaRevenue` to reconstruct the revenue _before_ the new store moved in, determine the previous revenue `train['counterFactualRevenue'] = train['totalRevenue']/(1 + train['deltaRevenue']/100)`. Then we could use estimates of the existing store performance for the feature `counterFactualRevenue`, and we don't have `totalRevenue` as a feature at all. We would also do this transformation on the validation set (it looks circular but isn't as long as we don't use `totalRevenue` -- it is assuming the way we would use this is projecting the current revenue of the store in the absence of the change).\n",
    "  - If it for the transactions of the _new store_, we still don't have the data, but we could use the information strategically. For example, if we know that we need to do at least 100 transactions for the new store to be viable, it makes sense to ask \"we are going to put a store here, and have at least 100 transactions in it\" to assess the impact and check we do not cannibalise the existing store too much.\n",
    "  - At the moment, I will include these features, as it isn't clear what the meaning of them are, so I will allow myself to include them. I am just including notes of how we would modify these fields\n",
    "3. We should check for colinearity in general, but in particular between `transactCount` and `totalRevenue` (we would expect Revenue and Transactions to be strongly correclated!)\n",
    "4. We have masked the effect of time -- a lot of the scatter can come about by looking at the same store on different days (especially for `region`, `Parking_slots` , and `Seats`)\n",
    "5. `totalActivitiesRefcircle` and `totalCustomerRefcircle` also have roughly the same shape as `transactCount` and `totalRevenue` (at least just looking at it on this scale).\n",
    "\n",
    "Let's look at possible correlations between `totalActivitiesRefcircle`, `totalCustomerRefcircle`, `transactCount`, and `totalRevenue`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-03-31T05:36:05.710Z"
    }
   },
   "outputs": [],
   "source": [
    "sns.pairplot(data=train[['totalActivitiesRefcircle', 'totalCustomersRefcircle', \n",
    "                         'transactCount', 'totalRevenue']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see activities and customers are both strongly correlated, so we should not include both of these features. We also see a moderate correlation between `transactCount` and `totalRevenue`. Finally, we also see that each of these distribution is right-skewed (i.e. the right tail is longer), making them good candidates for logging.\n",
    "\n",
    "This also makes sense, as we are interested in percentage variation in revenue (relative changes are multiplicative in `totalRevenue`, but _additive_ on `log(revenue)`). \n",
    "\n",
    "We would also expect things like\n",
    "- day of week\n",
    "- holiday events\n",
    "to affect the revenue by a mulplicative factor rather than an additive one (e.g. it seems reasonable that Monday's sales would be 20% higher than Wednesday's same-store sales, rather than $10k better regardless of the size of the store).\n",
    "\n",
    "Let's check that now by looking at time-series for a couple of the stores:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-03-31T05:36:05.712Z"
    }
   },
   "outputs": [],
   "source": [
    "train[train.store_id==(8,0,105)].plot('date', 'totalRevenue')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-03-31T05:36:05.714Z"
    }
   },
   "outputs": [],
   "source": [
    "train[train.store_id==(1, 31, 120)].plot('date', 'totalRevenue')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both of these randomly chosen stores look reasonably stationary (no long term trend). The revenue scales are very very different, but it is reasonable to think that we have a certain fraction of sales on different ways of the week (e.g. Monday's are 20% higher than Wednesdays, but Tuesdays are 10% lower). It doesn't look reasonable from the graph to assume this happens in absolute values.\n",
    "\n",
    "The no long term trend certainly makes modeling easier. Let's try logging the variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-03-31T05:36:05.717Z"
    }
   },
   "outputs": [],
   "source": [
    "train['log_revenue'] = np.log1p(train['totalRevenue'])\n",
    "train['log_activity'] = np.log1p(train['totalActivitiesRefcircle'])\n",
    "train['log_transaction'] = np.log1p(train['transactCount'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-03-31T05:36:05.718Z"
    }
   },
   "outputs": [],
   "source": [
    "sns.pairplot(data=train[['log_revenue', 'log_activity', 'log_transaction', 'deltaRevenue']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also see post-transformation that a lot of the same information is contained in the transaction and revenue data, so we should only keep one (and -- as argued above -- possibly neither if we are concerned about leakage).\n",
    "\n",
    "Let's see how this varies by store:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-03-31T05:36:05.721Z"
    }
   },
   "outputs": [],
   "source": [
    "sns.pairplot(data=train[['store_id', 'log_revenue', 'log_activity', 'log_transaction', 'deltaRevenue']],\n",
    "            hue='store_id')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a few stores here , but interestingly we can tell that each store has a distinct transaction and revenue amount, but the delta revenue has a similar shape for all the different stores. This suggests we would not lose a lot by standard scaling the different transaction data (where we do the standard scaling store-by-store)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How does store type matter\n",
    "\n",
    "We will have new stores in the validation set, but we know things about the stores (e.g. number of parking slots, number of seats, market type, etc). How does the distribution change for different attributes? e.g. can we predict revenue based on the parking slots? The seats? The region?\n",
    "\n",
    "Let's start with market type:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-03-31T05:36:05.724Z"
    }
   },
   "outputs": [],
   "source": [
    "sns.pairplot(data=train[['mtype', 'log_revenue', 'log_activity', 'log_transaction', 'deltaRevenue']],\n",
    "            hue='mtype')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, that's interesting -- we see a difference in the distribution of `deltaRevenue` based on market type **and** we see different distributions of transaction and revenue data, suggesting we keep both. The activity is more or less unaffected by market type. \n",
    "\n",
    "The observation of the different revenue and transaction data is particularly useful **if** we decide that we cannot use\n",
    "\n",
    "Let's do the same for `type_dtsf`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-03-31T05:36:05.727Z"
    }
   },
   "outputs": [],
   "source": [
    "sns.pairplot(data=train[['type_dtsf', 'log_revenue', 'log_activity', 'log_transaction', 'deltaRevenue']],\n",
    "            hue='type_dtsf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Except in transaction data, we don't see much difference. We also know `Mall` and `FoodC` are pretty small categories, so we should be suspicious of inference we draw. In a more involved project, we could use a [James-Stein encoder](https://kiwidamien.github.io/james-stein-encoder.html) (personal blog post). In this case, we will drop `type_dtsf` from the model.\n",
    "\n",
    "**Note**\n",
    "\n",
    "Because of lack of time, I have not verified that each market type and each `type_dtsf` has the same distribution of days of the week, and of holidays, which could potentially skew results. This would be something I would look at more closely in a real project.\n",
    "\n",
    "Finally, let's look at the region:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-03-31T05:36:05.730Z"
    }
   },
   "outputs": [],
   "source": [
    "train.head().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-03-31T05:36:05.732Z"
    }
   },
   "outputs": [],
   "source": [
    "sns.pairplot(data=train[['type_dtsf', 'log_revenue', 'log_activity', 'log_transaction', 'deltaRevenue', 'region']],\n",
    "            hue='region')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is significant information contained in the region (the distributions look slightly different in terms of where the mode is). This is probably worth keeping _provided that we are making predictions within the same set of regions_. We want to be a little cautious, as we only have a few stores per region, so there is a lot of variability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**bottom line:**\n",
    "\n",
    "1. We have seen that logging the revenue, activity, and transactions leads to non-skewed distributions\n",
    "2. Particularly for revenue and transactions, it would make sense for other features like days of the week to affect this multiplicatively (same % effect, different absolute effects). This becomes an additive effect on the log. \n",
    "3. We have seen that the activity and customers are strongly correlated, and kept only activity\n",
    "4. We have a hierarichal model, where we first have the information about the store (sample of 25), and then once we have the store we use the 100-300 points in time to make a prediction about `deltaRevenue`. We see that `mtype` seems to be relatively informative, but `type_dtsf` less so.\n",
    "5. Region carries data! We only have a few stores in any given region (and only 25 stores overall), so we want to encode this information in a way that \"shrinks\" the value back toward the average."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $\\S$ 1.6: Day of week effects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can ask how the day of week affects the delta Revenue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-03-31T05:36:05.738Z"
    }
   },
   "outputs": [],
   "source": [
    "sns.boxplot(x=\"dayname\", y=\"deltaRevenue\",\n",
    "            data=train)\n",
    "sns.despine(offset=10, trim=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see surprisingly little variatoin between days; Monday has slightly more variation and Saturday has less. What about holidays? How frequent are they?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-03-31T05:36:05.740Z"
    }
   },
   "outputs": [],
   "source": [
    "train.isholiday.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is a relatively small fraction of data, so unlikely to have a huge effect. Let's see, anyway:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-03-31T05:36:05.743Z"
    }
   },
   "outputs": [],
   "source": [
    "sns.boxplot(x=\"dayname\", y=\"deltaRevenue\",\n",
    "            hue='isholiday',\n",
    "            data=train)\n",
    "sns.despine(offset=10, trim=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Holidays are actually pretty important, but easy to get washed out in regularization due to it applying to a relatively small set of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $\\S$ 2: Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $\\S$ 2.1: Making the features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to simplify things, let's assume the features are fixed (i.e. we know days of the week are Mon-Sun, and let's assume that we have agreed on a standard for what the `mtypes` should be).\n",
    "\n",
    "If we **don't** assume this, we have to do our encoding as part of the cross fold validation (because we have to allow for unknown/new categories). This is slightly less robust to data changes, but louder about unexpected data (e.g. we will catch a day of the week being \"sunday\" instead of \"Sun\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-03-31T05:36:05.748Z"
    }
   },
   "outputs": [],
   "source": [
    "train.select_dtypes(include=[object,]).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Drop weekend (redundent with dayname)\n",
    "* Drop date\n",
    "* Drop `type_dtsf` as per discussion above (a more complicated model would use it with a James-Stein encoder)\n",
    "* One-hot encode `mtype`, `dayname`, `region`, and `directionCode` (the last two are numeric already, but they should not be ordered!)\n",
    "* Turn `isholiday` into a integer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-03-31T05:36:05.750Z"
    }
   },
   "outputs": [],
   "source": [
    "def load_data(path='data/train.csv', exclude_outputs_greater_than=300) -> pd.DataFrame:\n",
    "    df = pd.read_csv(path)\n",
    "    df = df[df.deltaRevenue < exclude_outputs_greater_than]\n",
    "    df['log_revenue'] = np.log1p(df['totalRevenue'])\n",
    "    df['log_transaction'] = np.log1p(df['transactCount'])\n",
    "    df['log_activities'] = np.log1p(df['totalActivitiesRefcircle'])\n",
    "    \n",
    "    df['store_id'] = df.apply(lambda row: (row.region, row.Parking_slots, row.Seats), axis=1)\n",
    "    \n",
    "    df['isholiday'] = (train['isholiday']=='yes').astype(int)\n",
    "    \n",
    "    to_drop = [\n",
    "        'totalRevenue', 'transactCount', 'totalActivitiesRefcircle', 'totalCustomersRefcircle',\n",
    "        'weekend', 'type_dtsf', 'Parking_slots', 'Seats', 'date']\n",
    "    return df.drop(to_drop, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-03-31T05:36:05.752Z"
    }
   },
   "outputs": [],
   "source": [
    "# Reload the training data and start from scratch\n",
    "\n",
    "train = load_data(path='data/train.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-03-31T05:36:05.754Z"
    }
   },
   "outputs": [],
   "source": [
    "ohe_encoder = ce.OneHotEncoder(cols=['dayname', 'mtype', 'region', 'directionCode'],\n",
    "                               use_cat_names=True,\n",
    "                               return_df=True)\n",
    "store_id_encoder = ce.OrdinalEncoder(cols=['store_id'], return_df=True)\n",
    "train = store_id_encoder.fit_transform(train)\n",
    "train = ohe_encoder.fit_transform(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-03-31T05:36:05.757Z"
    }
   },
   "outputs": [],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`store_id` is a number and isn't being used as a feature -- instead I am only using it to specify my groups for cross-fold validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $\\S$ 2.2: preprocessing pipeline and KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-03-31T05:36:05.761Z"
    }
   },
   "outputs": [],
   "source": [
    "scale_pipeline = Pipeline([\n",
    "    ('scale', StandardScaler())\n",
    "])\n",
    "\n",
    "preprocess_pipeline = ColumnTransformer([\n",
    "    ('scale', scale_pipeline, ['log_revenue', 'log_activities', 'log_transaction'])\n",
    "], remainder='passthrough')\n",
    "\n",
    "\n",
    "folds = GroupKFold(n_splits=5)\n",
    "groups = train.store_id\n",
    "\n",
    "X, y = train.drop(['deltaRevenue', 'store_id'], axis=1), train.deltaRevenue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-03-31T05:36:05.764Z"
    }
   },
   "outputs": [],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $\\S$3 : Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $\\S$3.1: A simple Lasso model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start with one of the simplest models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-03-31T05:36:05.767Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "model_pipeline = Pipeline([\n",
    "    ('preprocessing', preprocess_pipeline),\n",
    "    ('lasso', (Lasso(max_iter=100_000)))\n",
    "])\n",
    "\n",
    "param = {\n",
    "    'lasso__alpha': [1e-2, 1e-1, 3e-1, 1, 3, 10, 30]\n",
    "}\n",
    "\n",
    "grid = GridSearchCV(model_pipeline, param_grid=param, cv=folds.split(X, y, groups)).fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-03-31T05:36:05.769Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.plot([p['lasso__alpha'] for p in grid.cv_results_['params']],\n",
    "         grid.cv_results_['mean_test_score'])\n",
    "plt.xscale('log')\n",
    "plt.xlabel('$\\\\alpha$')\n",
    "plt.ylabel('$R^2$')\n",
    "plt.title('Results on validation set (KFold Cross Validation)');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-03-31T05:36:05.771Z"
    }
   },
   "outputs": [],
   "source": [
    "grid.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-03-31T05:36:05.773Z"
    }
   },
   "outputs": [],
   "source": [
    "grid.best_estimator_.named_steps['lasso'].coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-03-31T05:36:05.775Z"
    }
   },
   "outputs": [],
   "source": [
    "important_features = pd.DataFrame(zip(grid.best_estimator_.named_steps['lasso'].coef_, X.columns), \n",
    "                                 columns=['coef', 'feature'])\n",
    "important_features[abs(important_features.coef)>0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The important features that are not constant across a store are\n",
    "- $r_{12}$\n",
    "- whether it is Monday or Saturday (which makes sense given what we saw in distributions for days of the week)\n",
    "- $\\log(\\text{revenue})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $\\S$3.2: Let's look at feature importances from a tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see if a RandomForest regressor gives similar results, especially for which features are important. Note that we should avoid using the standard feature importance, especially as we have one-hot-encoded features (see [this article](https://explained.ai/rf-importance/)). We do not have to worry about scaling for the tree-based methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-03-31T05:36:05.778Z"
    }
   },
   "outputs": [],
   "source": [
    "# Warning -- this cell takes \n",
    "\n",
    "param_tree = {\n",
    "    'max_depth': [30, 100, 150],\n",
    "    'max_features': [10, 11, 12, 13, 14],\n",
    "    'min_samples_leaf': [3, 4, 5],\n",
    "    'min_samples_split': [8, 10, 12],\n",
    "    'n_estimators': [80, 100, 200, 300]\n",
    "}\n",
    "\n",
    "\n",
    "# Create a based model\n",
    "rf = RandomForestRegressor()\n",
    "\n",
    "\n",
    "grid_tree = GridSearchCV(estimator=rf, \n",
    "                         param_grid=param_tree, \n",
    "                         cv=folds.split(X, y, groups)).fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-03-31T05:36:05.780Z"
    }
   },
   "outputs": [],
   "source": [
    "grid_tree.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's check the sensitivity of the score to the parameters used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-03-31T05:36:05.783Z"
    }
   },
   "outputs": [],
   "source": [
    "grid_tree.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look along each dimension / parameter value and plot the score as we change the values (holding the others constant)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-03-31T05:36:05.785Z"
    }
   },
   "outputs": [],
   "source": [
    "df_parameters = pd.DataFrame(grid_tree.cv_results_)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-03-31T05:36:05.787Z"
    }
   },
   "outputs": [],
   "source": [
    "def plot_scan_parameter(param_to_scan, best_params, df_parameters=df_parameters):\n",
    "    \"\"\"Sensitivity analysis for hyper-parameters\n",
    "    \n",
    "    Holds the other hyperparameters constant at their optimal values,\n",
    "    and scans df_parameters to see how the mean score (R^2) changed across\n",
    "    as we moved the `param_to_scan`.\n",
    "    \n",
    "    Also includes the 1-sigma variation in the score from the changes we\n",
    "    see looking at different folds\n",
    "    \"\"\"\n",
    "    mask = pd.Series(True, index=df_parameters.index)\n",
    "    column_name = f'param_{param_to_scan}'\n",
    "    \n",
    "    for key, value in best_params.items():\n",
    "        if key==param_to_scan:\n",
    "            continue\n",
    "        mask = mask & (df_parameters[f'param_{key}']==value)\n",
    "    df_selection = df_parameters[mask].sort_values(column_name).copy()\n",
    "    df_selection.plot(x=column_name, y='mean_test_score')\n",
    "    df_selection[column_name] = df_selection[column_name].astype(float)\n",
    "    plt.fill_between(x=df_selection[column_name],\n",
    "                     y1=df_selection['mean_test_score'] + df_selection['std_test_score'],\n",
    "                     y2=df_selection['mean_test_score'] - df_selection['std_test_score'],\n",
    "                     alpha=0.3\n",
    "                    )\n",
    "    plt.ylabel('average OOS R2')\n",
    "    plt.title(f'R2 OOS changes vs parameter {param_to_scan}')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-03-31T05:36:05.789Z"
    }
   },
   "outputs": [],
   "source": [
    "plot_scan_parameter('n_estimators', grid_tree.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-03-31T05:36:05.791Z"
    }
   },
   "outputs": [],
   "source": [
    "plot_scan_parameter('max_depth', grid_tree.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-03-31T05:36:05.793Z"
    }
   },
   "outputs": [],
   "source": [
    "plot_scan_parameter('min_samples_leaf', grid_tree.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-03-31T05:36:05.795Z"
    }
   },
   "outputs": [],
   "source": [
    "plot_scan_parameter('min_samples_split', grid_tree.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-03-31T05:36:05.797Z"
    }
   },
   "outputs": [],
   "source": [
    "plot_scan_parameter('max_features', grid_tree.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that we are not incredibly sensitive to the values used (especially when compared to the size of the 1-$\\sigma$ error bars), so we don't gain a lot by altering the hyper parameters more.\n",
    "\n",
    "Let's look at what the important features are (using permutation importance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-03-31T05:36:05.800Z"
    }
   },
   "outputs": [],
   "source": [
    "importances = permutation_importance(grid_tree, X, y, n_repeats=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-03-31T05:36:05.802Z"
    }
   },
   "outputs": [],
   "source": [
    "for index in importances.importances_mean.argsort()[::-1]:\n",
    "    # are we outside of a one-sigma error?\n",
    "    if importances.importances_std[index] < abs(importances.importances_mean[index]):\n",
    "        print(f'{X.columns[index]}\\t\\t{importances.importances_mean[index]:.2f} +/- {importances.importances_std[index]:.2f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the std dev in the different folds?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-03-31T05:36:05.804Z"
    }
   },
   "outputs": [],
   "source": [
    "index_best = grid_tree.cv_results_['rank_test_score'].argmin()\n",
    "print(f'Best OOS mean score: {grid_tree.cv_results_[\"mean_test_score\"][index_best]:.4f} with '\n",
    "      f'standard dev {grid_tree.cv_results_[\"std_test_score\"][index_best]:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $\\S$3.3: Model reducing the unnecessary features of model 3.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see there is a really high variance in the models in section 3.2 when looking at the standard error of the different folds. Let's eliminate some of the less useful variables in our model, and refit.\n",
    "\n",
    "We will eliminate features with permuation importance less than 0.02 as a starting point (normally we would eliminate features one at a time, as features could have low importance due to colinearity and they end up splitting importance between them, but in the interest of time we make this shortcut)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-03-31T05:36:05.808Z"
    }
   },
   "outputs": [],
   "source": [
    "features_to_keep = [c for c, importance in zip(X.columns, importances.importances_mean) if importance > 0.02]\n",
    "features_to_keep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-03-31T05:36:05.810Z"
    }
   },
   "outputs": [],
   "source": [
    "X_2 = X[features_to_keep].copy()\n",
    "y_2 = y.copy()  # not strictly necessary, but makes easier to read/follow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-03-31T05:36:05.811Z"
    }
   },
   "outputs": [],
   "source": [
    "# Warning -- this cell takes \n",
    "\n",
    "param_tree = {\n",
    "    'max_depth': [30, 100, 150],\n",
    "    'max_features': [10, 11, 12, 13, 14],\n",
    "    'min_samples_leaf': [3, 4, 5],\n",
    "    'min_samples_split': [8, 10, 12],\n",
    "    'n_estimators': [80, 100, 200, 300]\n",
    "}\n",
    "\n",
    "\n",
    "# Create a based model\n",
    "rf = RandomForestRegressor()\n",
    "\n",
    "\n",
    "grid_tree_reduced = GridSearchCV(estimator=rf,\n",
    "                                 param_grid=param_tree, \n",
    "                                 cv=folds.split(X_2, y_2, groups)).fit(X_2, y_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-03-31T05:36:05.813Z"
    }
   },
   "outputs": [],
   "source": [
    "grid_tree_reduced.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-03-31T05:36:05.815Z"
    }
   },
   "outputs": [],
   "source": [
    "grid_tree_reduced.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What was the variation?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-03-31T05:36:05.818Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "index_of_best = grid_tree_reduced.cv_results_['rank_test_score'].argmin()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-03-31T05:36:05.819Z"
    }
   },
   "outputs": [],
   "source": [
    "grid_tree_reduced.cv_results_['std_test_score'][index_of_best]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see we still have high variation across folds, and have no really solved the problem we saw in the previous model, so there is not a compelling reason to switch. The variance is slightly worse, and the mean is slightly worse, but the benefit of a simpler model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $\\S$3.4: Model encoding the categorical variables using target encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we have one-hot encoded the region, which seems like an important feature (at least region 2 and region 8 showed as significant, and many region variables showed up as important in the Lasso model).\n",
    "\n",
    "Let's also just double check the number of stores in each region. Usually you would do this on a holdout sample, but here we have stores in a single region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-03-31T05:36:05.823Z"
    }
   },
   "outputs": [],
   "source": [
    "train.groupby('region').store_id.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-03-31T05:36:05.825Z"
    }
   },
   "outputs": [],
   "source": [
    "train.groupby('region').deltaRevenue.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What stops us from applying something that `ce.TargetEncoder` is the repeated data. We have many rows of data corresponding to the same store, and `ce.TargetEncoder` treats these as if they were independent! This is pretty hard to cross-validate, as some regions only have one store. For example, the $-10$ points in region 10 doesn't really tell us about region 10; it tells us about the single store in region 10 in our dataset (it might be that store is bad, rather than a competitive region).\n",
    "\n",
    "With so few stores in a region, I am worried about overfitting - a more sophisticated model (with more time) would try to do proper hierarchical modeling of stores on regions. So sadly I will be deleting region (accepting slightly more variance and reducing bias).\n",
    "\n",
    "I do feel a little more comfortable with the market type:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-03-31T05:36:05.828Z"
    }
   },
   "outputs": [],
   "source": [
    "train.groupby('mtype').agg({'store_id': 'nunique', 'deltaRevenue': 'mean'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The shifts are not as big as region, but it is easier to validate / extend. We also have for `directionCode`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-03-31T05:36:05.830Z"
    }
   },
   "outputs": [],
   "source": [
    "train.groupby('directionCode').agg({'store_id': 'nunique', 'deltaRevenue': 'mean'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### A shortcut (i.e. technically wrong)\n",
    "\n",
    "We still cannot do naive cross validation by row. Instead we need to do target encoding by the individual store, so the _right_ way of doing it is, in each fold,\n",
    "1. Find the mean for each store of the target\n",
    "2. Find the std dev between the means of stores in the same `mtype` (or `directionCode`). This will have approximately 20 rows (4 out of 5 folds, where each store appears in only one fold)\n",
    "3. Train the target encoder on the 20 rows. It will shrink the values toward the global mean of the target (the more variation there is between the mean of stores in the same `mtype`, the more weight the global mean carries)\n",
    "4. Apply the encoder to the full data frame with many repeated rows\n",
    "\n",
    "It is hard to fit this into GridSearchCV, so I'd have to do something custom or write my own Pipeline stage (or use PyMC3). \n",
    "\n",
    "So the method below is pragmatic, but does incur data leakage!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-03-31T05:36:05.833Z"
    }
   },
   "outputs": [],
   "source": [
    "train = load_data('data/train.csv')\n",
    "train = store_id_encoder.transform(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-03-31T05:36:05.835Z"
    }
   },
   "outputs": [],
   "source": [
    "mtype_revenue_by_store = train.groupby(['mtype', 'store_id']).deltaRevenue.mean()\n",
    "mtype_revenue_by_store"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Within each group of `mtype`, we will calculate the mean and standard deviation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-03-31T05:36:05.837Z"
    }
   },
   "outputs": [],
   "source": [
    "mtype_summary = mtype_revenue_by_store.reset_index().groupby('mtype').agg({'deltaRevenue':['mean', 'std'], 'store_id': 'count'})\n",
    "mtype_summary.columns=['mean', 'std', 'n_stores']\n",
    "mtype_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's get the overall mean and std:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-03-31T05:36:05.840Z"
    }
   },
   "outputs": [],
   "source": [
    "DELTA_REV_MEAN_ACROSS_STORES = mtype_revenue_by_store.mean()\n",
    "DELTA_REV_STD_IN_MEAN_ACROSS_STORES = mtype_revenue_by_store.std()\n",
    "\n",
    "print(f\"Mean ={DELTA_REV_MEAN_ACROSS_STORES:.2f}, stddev={DELTA_REV_STD_IN_MEAN_ACROSS_STORES:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we apply the shinkage trick, as outlined [here](https://kiwidamien.github.io/derivations-and-conjugate-priors-average-ratings.html). Again, the intuition is that if the variance within a group is large (e.g. Rural) we bias toward the global average. If it is small, we bias toward the group average. Here \"big\" and \"small\" are made by comparing to the average of all stores (disregarding the `mtype`).\n",
    "\n",
    "We have\n",
    "\n",
    "$$\\text{encoded value for cat $i$} = B_i \\bar{x_i} + (1-B_i)\\mu, \\quad\\quad B_i = \\frac{\\tau^2}{\\tau^2 + (\\sigma_i^2/\\text{n_stores})}$$\n",
    "\n",
    "where $\\bar{x}_i$ is the mean for category $i$ (across stores), $\\mu$ is the global mean, $\\tau$ is the global std dev, and $\\sigma_i$ is the std dev in the target for that store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-03-31T05:36:05.842Z"
    }
   },
   "outputs": [],
   "source": [
    "mtype_summary['B'] = (mtype_summary.apply(\n",
    "    lambda row: DELTA_REV_STD_IN_MEAN_ACROSS_STORES**2/(DELTA_REV_STD_IN_MEAN_ACROSS_STORES**2 + (row['std']**2/row.n_stores)),\n",
    "    axis=1)\n",
    "                     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-03-31T05:36:05.844Z"
    }
   },
   "outputs": [],
   "source": [
    "mtype_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that `Hyper` and `Urban` (with lower std than the entire population) have $B>0.5$, so the mean is weighted more toward the group value. Working as expected!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-03-31T05:36:05.847Z"
    }
   },
   "outputs": [],
   "source": [
    "mtype_summary['encoded'] = mtype_summary['B']*mtype_summary['mean'] + (1-mtype_summary['B'])*DELTA_REV_MEAN_ACROSS_STORES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-03-31T05:36:05.849Z"
    }
   },
   "outputs": [],
   "source": [
    "mtype_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's use this encoded value in the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-03-31T05:36:05.851Z"
    }
   },
   "outputs": [],
   "source": [
    "train['encoded_mtype'] = train.mtype.apply(lambda value: mtype_summary.loc[value, 'encoded'])\n",
    "train = train.drop('mtype', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-03-31T05:36:05.853Z"
    }
   },
   "outputs": [],
   "source": [
    "# We also only saw importance for Sun, Mon, Tue\n",
    "train['is_mon'] = (train['dayname']=='Mon').astype(int)\n",
    "train['is_tue'] = (train['dayname']=='Tue').astype(int)\n",
    "train['is_wed'] = (train['dayname']=='Wed').astype(int)\n",
    "train = train.drop('dayname', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-03-31T05:36:05.854Z"
    }
   },
   "outputs": [],
   "source": [
    "train = train.drop(['store_id', 'directionCode', 'region'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-03-31T05:36:05.856Z"
    }
   },
   "outputs": [],
   "source": [
    "X_3, y_3 = train.drop(['deltaRevenue'], axis=1), train.deltaRevenue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use our previous best fit features to make a starting point here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-03-31T05:36:05.858Z"
    }
   },
   "outputs": [],
   "source": [
    "param_tree = {\n",
    "    'max_depth': [100, 150, 200],\n",
    "    'max_features': [13, 14, 15, 16],\n",
    "    'min_samples_leaf': [5, 8, 10],\n",
    "    'min_samples_split': [8, 10, 12],\n",
    "    'n_estimators': [80, 100, 200]\n",
    "}\n",
    "# Create a based model\n",
    "rf = RandomForestRegressor()\n",
    "\n",
    "\n",
    "grid_tree_encoded = GridSearchCV(estimator=rf, \n",
    "                                 param_grid=param_tree, \n",
    "                                 cv=folds.split(X_3, y_3, groups)).fit(X_3, y_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-03-31T05:36:05.860Z"
    }
   },
   "outputs": [],
   "source": [
    "grid_tree_encoded.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-03-31T05:36:05.862Z"
    }
   },
   "outputs": [],
   "source": [
    "grid_tree_encoded.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our method to encode the market type actually made the score (significantly) worse, so let's stay with the original tree model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $\\S$3.5: summary of models\n",
    "\n",
    "We tried three models, all selected with the goal of being able to do feature reduction. The models were:\n",
    "\n",
    "\n",
    "1. **LASSO** simple Lasso, with cross-validation to determine regularization\n",
    "2. **Basic forest**, a random forest, with categorical variables one-hot encoded\n",
    "3. **Reduced forest**, a random forest on a reduced set of features, to try and counteract the wide variance seen between folds in model 2\n",
    "4. **Advanced forest**, a random tree, using feature selection (via 2) and trying to use the target to determine the encoding for the market type (done in a way that leaked data).\n",
    "\n",
    "I found model 2 performed the best. Model 3 was similar, with slightly fewer parameters, but both models 2 and 3 had large variations between the folds. They were statistically indistinguishable; I kept model 2 because I had built out more of the pipeline with it in mind.\n",
    "\n",
    "The extra information from encoding the market category in model 4 made the model worse. This suggests that we really were encoding information about the specific stores when training, and we cannot use this feature without doing a lot more modeling.\n",
    "\n",
    "We move to validation with the **Basic forest**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $\\S$4. Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that by default, `GridSearchCV` has `refit=True`, and fits the best parameter on _all_ the training data. However, it is better to ensure that we have prepared the data identically, so we will retrain the model using the best parameters for `grid_tree`.\n",
    "\n",
    "Let's reload the data and train on everything, with a reproducible data pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-03-31T05:36:05.866Z"
    }
   },
   "outputs": [],
   "source": [
    "def prepare_data(path, exclude_outputs_greater_than=float('inf')):\n",
    "    df = load_data(path, exclude_outputs_greater_than=exclude_outputs_greater_than)\n",
    "    print(f\"Highest magnitude still in output: {abs(df.deltaRevenue).max():,.2f}\")\n",
    "    df = ohe_encoder.transform(df)\n",
    "    df = df.drop(['store_id'], axis=1)\n",
    "    # Note that tree-based methods don't need standard scaling\n",
    "    \n",
    "    return df.drop(['deltaRevenue'], axis=1), df.deltaRevenue\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-03-31T05:36:05.869Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train, y_train = prepare_data(path='data/train.csv', exclude_outputs_greater_than=300)\n",
    "\n",
    "# Let's not cut off outliers in the validation set, unless it is obviously a typo of some sort!\n",
    "X_val, y_val = prepare_data(path='data/validation.csv', exclude_outputs_greater_than=float('inf'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-31T02:57:30.864344Z",
     "start_time": "2021-03-31T02:57:30.826374Z"
    }
   },
   "source": [
    "Okay, this looks good. We should also check we have the same columns, and in the same order!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-03-31T05:36:05.871Z"
    }
   },
   "outputs": [],
   "source": [
    "def column_check(X_train, X_val):\n",
    "    newline = \"\\n\"\n",
    "    mismatches = [f\"col #{index}: {c_train} <--> {c_val}\" \n",
    "                  for index, (c_train, c_val) in enumerate(zip(X_train.columns, X_val.columns))\n",
    "                  if c_train != c_val\n",
    "                 ]\n",
    "    msg = (\"Misaligned columns check:\\n\" + \n",
    "           \"\\n\".join(mismatches))\n",
    "    return msg\n",
    "\n",
    "\n",
    "print(column_check(X_train, X_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, that is really frustrating that the columns come in different orders. At least we have the same labels, just permuted. Let's put validation in the same order as training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-03-31T05:36:05.874Z"
    }
   },
   "outputs": [],
   "source": [
    "X_val = X_val[X_train.columns]\n",
    "print(column_check(X_train, X_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yay, all columns are aligned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-03-31T05:36:05.876Z"
    }
   },
   "outputs": [],
   "source": [
    "final_model = RandomForestRegressor(**grid_tree.best_params_)\n",
    "final_model.fit(X_train, y_train)\n",
    "final_model.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-03-31T05:36:05.878Z"
    }
   },
   "outputs": [],
   "source": [
    "final_model.score(X_val, y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $\\S$ 5: Improvements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned a few times, one of the most challenging things about this dataset is that the rows were related because there were a few individual stores. Taking a standard approach, where the rows were treated separately, would not be appropriate, and would lead us to be overly confident in our results.\n",
    "\n",
    "In particular, features like seats, parking spaces, market type, store type, and the region are repeated in a completely correlated way (there are only 25 unique combinations) so we can end up encoding a lot of information in these features that are encodings about the store, and would not generalize to new stores.\n",
    "\n",
    "The way to approach this problem is probably to do a hierarichal model on the parameters that describe a store. I did a high level of this approach when encoding the `mtype` variable. To do it in a statistically robust way would be to set a prior on the distribution of these variables and the average outcome. We would replace all the features that were constant on a particular store with the output of this model as a feature in the time dependent set.\n",
    "\n",
    "When given a new store, we would be able to calculate the store column. Because we only have 25 stores, we would need a fairly simple model, and would use statistical techniques (e.g. AIC or WAIC) to determine which features to keep, rather than using cross validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hela",
   "language": "python",
   "name": "hela"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
